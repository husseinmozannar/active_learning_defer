{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7Aympk-qwGu"
      },
      "source": [
        "# Preliminaries: model definition and utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cFhWBUzvqmZF"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import random\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "import argparse\n",
        "import os\n",
        "import shutil\n",
        "import time\n",
        "import torch.nn.parallel\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.optim\n",
        "import torch.utils.data\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data.dataset import random_split\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iYllTn0F2iDg"
      },
      "source": [
        "# Neural Network definitions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "plcXrgr8q_dR"
      },
      "source": [
        "WideResNet from following [repo](https://github.com/xternalz/WideResNet-pytorch/blob/master/wideresnet.py)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qP2NJY4BrEH8"
      },
      "outputs": [],
      "source": [
        "class BasicBlock(nn.Module):\n",
        "    def __init__(self, in_planes, out_planes, stride, dropRate=0.0):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
        "        self.relu1 = nn.ReLU(inplace=True)\n",
        "        self.conv1 = nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
        "                               padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_planes)\n",
        "        self.relu2 = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv2d(out_planes, out_planes, kernel_size=3, stride=1,\n",
        "                               padding=1, bias=False)\n",
        "        self.droprate = dropRate\n",
        "        self.equalInOut = (in_planes == out_planes)\n",
        "        self.convShortcut = (not self.equalInOut) and nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride,\n",
        "                                                                padding=0, bias=False) or None\n",
        "    def forward(self, x):\n",
        "        if not self.equalInOut:\n",
        "            x = self.relu1(self.bn1(x))\n",
        "        else:\n",
        "            out = self.relu1(self.bn1(x))\n",
        "        out = self.relu2(self.bn2(self.conv1(out if self.equalInOut else x)))\n",
        "        if self.droprate > 0:\n",
        "            out = F.dropout(out, p=self.droprate, training=self.training)\n",
        "        out = self.conv2(out)\n",
        "        return torch.add(x if self.equalInOut else self.convShortcut(x), out)\n",
        "\n",
        "\n",
        "class NetworkBlock(nn.Module):\n",
        "    def __init__(self, nb_layers, in_planes, out_planes, block, stride, dropRate=0.0):\n",
        "        super(NetworkBlock, self).__init__()\n",
        "        self.layer = self._make_layer(block, in_planes, out_planes, nb_layers, stride, dropRate)\n",
        "    def _make_layer(self, block, in_planes, out_planes, nb_layers, stride, dropRate):\n",
        "        layers = []\n",
        "        for i in range(int(nb_layers)):\n",
        "            layers.append(block(i == 0 and in_planes or out_planes, out_planes, i == 0 and stride or 1, dropRate))\n",
        "        return nn.Sequential(*layers)\n",
        "    def forward(self, x):\n",
        "        return self.layer(x)\n",
        "\n",
        "\n",
        "class WideResNet(nn.Module):\n",
        "    def __init__(self, depth, num_classes, widen_factor=1, dropRate=0.0):\n",
        "        super(WideResNet, self).__init__()\n",
        "        nChannels = [16, 16 * widen_factor, 32 * widen_factor, 64 * widen_factor]\n",
        "        assert ((depth - 4) % 6 == 0)\n",
        "        n = (depth - 4) / 6\n",
        "        block = BasicBlock\n",
        "        # 1st conv before any network block\n",
        "        self.conv1 = nn.Conv2d(3, nChannels[0], kernel_size=3, stride=1,\n",
        "                               padding=1, bias=False)\n",
        "        # 1st block\n",
        "        self.block1 = NetworkBlock(n, nChannels[0], nChannels[1], block, 1, dropRate)\n",
        "        # 2nd block\n",
        "        self.block2 = NetworkBlock(n, nChannels[1], nChannels[2], block, 2, dropRate)\n",
        "        # 3rd block\n",
        "        self.block3 = NetworkBlock(n, nChannels[2], nChannels[3], block, 2, dropRate)\n",
        "        # global average pooling and classifier\n",
        "        self.bn1 = nn.BatchNorm2d(nChannels[3])\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.fc = nn.Linear(nChannels[3], num_classes)\n",
        "        self.nChannels = nChannels[3]\n",
        "        self.softmax = nn.Softmax()\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                m.bias.data.zero_()\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.block1(out)\n",
        "        out = self.block2(out)\n",
        "        out = self.block3(out)\n",
        "        out = self.relu(self.bn1(out))\n",
        "        out = F.avg_pool2d(out, 8)\n",
        "        out = out.view(-1, self.nChannels)\n",
        "        out = self.fc(out)\n",
        "        out = self.softmax(out)\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YeFb2p2TzjbF"
      },
      "outputs": [],
      "source": [
        "# simple conv network\n",
        "# (argument 2 of the first nn.Conv2d, and argument 1 of the second nn.Conv2d – they need to be the same number)\n",
        "class NetSimple(nn.Module):\n",
        "    def __init__(self, num_classes, width1 = 6, width2 = 16,ff_units1 = 120, ff_units2 = 84):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3, width1, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(width1, width2, 5)\n",
        "        self.fc1 = nn.Linear(width2 * 5 * 5, ff_units1)\n",
        "        self.fc2 = nn.Linear(ff_units1, ff_units2)\n",
        "        self.fc3 = nn.Linear(ff_units2, num_classes)\n",
        "        self.softmax = nn.Softmax()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        x = self.softmax(x)\n",
        "        return x\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# simple conv network\n",
        "# (argument 2 of the first nn.Conv2d, and argument 1 of the second nn.Conv2d – they need to be the same number)\n",
        "class NetComplex(nn.Module):\n",
        "    def __init__(self, num_classes, width1 = 6, width2 = 16,ff_units1 = 120, ff_units2 = 84):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3, width1, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(width1, width2, 5)\n",
        "        self.conv3 = nn.Conv2d(width2, width2, 5)\n",
        "        self.fc1 = nn.Linear(width2 * 3 * 3, ff_units1)\n",
        "        self.fc2 = nn.Linear(ff_units1, ff_units2)\n",
        "        self.fc3 = nn.Linear(ff_units2, num_classes)\n",
        "        self.softmax = nn.Softmax()\n",
        "\n",
        "    def forward(self, x):\n",
        "        to_print_size = False\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        if to_print_size:\n",
        "            print(x.size())\n",
        "        x = F.relu(self.conv2(x))\n",
        "        if to_print_size:\n",
        "            print(x.size())\n",
        "        x = self.pool(F.relu(self.conv3(x)))\n",
        "        if to_print_size:\n",
        "            print(x.size())\n",
        "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
        "        if to_print_size:\n",
        "            print(x.size())\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        x = self.softmax(x)\n",
        "        return x\n",
        "\n"
      ],
      "metadata": {
        "id": "jex4IogFu-c2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ll6QD_IrQeD"
      },
      "source": [
        "# Metrics and utilities"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def reject_CrossEntropyLoss(outputs, m, labels, m2, n_classes):\n",
        "    '''\n",
        "    The L_{CE} loss implementation for CIFAR\n",
        "    ----\n",
        "    outputs: network outputs\n",
        "    m: cost of deferring to expert cost of classifier predicting (I_{m =y})\n",
        "    labels: target\n",
        "    m2:  cost of classifier predicting (alpha* I_{m\\neq y} + I_{m =y})\n",
        "    n_classes: number of classes\n",
        "    '''\n",
        "    batch_size = outputs.size()[0]  # batch_size\n",
        "    rc = [n_classes] * batch_size\n",
        "    outputs = -m * torch.log2(outputs[range(batch_size), rc] + 1e-12) - m2 * torch.log2(\n",
        "        outputs[range(batch_size), labels] + 1e-12)  \n",
        "    return torch.sum(outputs) / batch_size + 1e-12\n",
        "\n",
        "def my_CrossEntropyLoss(outputs, labels):\n",
        "    # Regular Cross entropy loss\n",
        "    batch_size = outputs.size()[0]  # batch_size\n",
        "    outputs = - torch.log2(outputs[range(batch_size), labels])  # regular CE\n",
        "    return torch.sum(outputs) / batch_size\n"
      ],
      "metadata": {
        "id": "nKY9Bxfx2Xy4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m93DvFTXrRwt"
      },
      "outputs": [],
      "source": [
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "def accuracy(output, target, topk=(1,)):\n",
        "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
        "    maxk = max(topk)\n",
        "    batch_size = target.size(0)\n",
        "\n",
        "    _, pred = output.topk(maxk, 1, True, True)\n",
        "    pred = pred.t()\n",
        "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
        "\n",
        "    res = []\n",
        "    for k in topk:\n",
        "        correct_k = correct[:k].view(-1).float().sum(0)\n",
        "        res.append(correct_k.mul_(100.0 / batch_size))\n",
        "    return res\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def metrics_print(net, expert_fn, n_classes, loader):\n",
        "    '''\n",
        "    Computes metrics for deferal\n",
        "    -----\n",
        "    Arguments:\n",
        "    net: model\n",
        "    expert_fn: expert model\n",
        "    n_classes: number of classes\n",
        "    loader: data loader\n",
        "    '''\n",
        "    correct = 0\n",
        "    correct_sys = 0\n",
        "    exp = 0\n",
        "    exp_total = 0\n",
        "    total = 0\n",
        "    real_total = 0\n",
        "    alone_correct = 0\n",
        "    correct_pred = {classname: 0 for classname in cifar_classes}\n",
        "    total_pred = {classname: 0 for classname in cifar_classes}\n",
        "    with torch.no_grad():\n",
        "        for data in loader:\n",
        "            images, labels, _, _ ,_ = data\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = net(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            batch_size = outputs.size()[0]  # batch_size\n",
        "            exp_prediction = expert_fn(images, labels)\n",
        "            for i in range(0, batch_size):\n",
        "                r = (predicted[i].item() == n_classes)\n",
        "                prediction = predicted[i]\n",
        "                final_pred = 0\n",
        "                if predicted[i] == n_classes:\n",
        "                    max_idx = 0\n",
        "                    # get second max\n",
        "                    for j in range(0, n_classes):\n",
        "                        if outputs.data[i][j] >= outputs.data[i][max_idx]:\n",
        "                            max_idx = j\n",
        "                    prediction = max_idx\n",
        "                else:\n",
        "                    prediction = predicted[i]\n",
        "                alone_correct += (prediction == labels[i]).item()\n",
        "                if r == 0:\n",
        "                    total += 1\n",
        "                    final_pred = predicted[i]\n",
        "                    correct += (predicted[i] == labels[i]).item()\n",
        "                    correct_sys += (predicted[i] == labels[i]).item()\n",
        "                if r == 1:\n",
        "                    final_pred = exp_prediction[i]\n",
        "                    exp += (exp_prediction[i] == labels[i].item())\n",
        "                    correct_sys += (exp_prediction[i] == labels[i].item())\n",
        "                    exp_total += 1\n",
        "                real_total += 1\n",
        "                if labels[i].item() == final_pred:\n",
        "                    correct_pred[cifar_classes[labels[i].item()]] += 1\n",
        "                total_pred[cifar_classes[labels[i].item()]] += 1\n",
        "    cov = str(total) + str(\" out of\") + str(real_total)\n",
        "    to_print = {\"coverage\": cov, \"system accuracy\": 100 * correct_sys / real_total,\n",
        "                \"expert accuracy\": 100 * exp / (exp_total + 0.0002),\n",
        "                \"classifier accuracy\": 100 * correct / (total + 0.0001),\n",
        "                \"alone classifier\": 100 * alone_correct / real_total}\n",
        "    print(to_print)\n",
        "    for classname, correct_count in correct_pred.items():\n",
        "        accuracy = 100 * float(correct_count) / total_pred[classname]\n",
        "        print(\"Accuracy for class {:5s} is: {:.3f} %\".format(classname,\n",
        "                                                    accuracy))\n",
        "    return to_print\n",
        "def metrics_print_oracle(net_class, expert_fn, expert_k, n_classes, loader):\n",
        "    correct = 0\n",
        "    correct_sys = 0\n",
        "    exp = 0\n",
        "    exp_total = 0\n",
        "    total = 0\n",
        "    real_total = 0\n",
        "    correct_pred = {classname: 0 for classname in cifar_classes}\n",
        "    total_pred = {classname: 0 for classname in cifar_classes}\n",
        "    with torch.no_grad():\n",
        "        for data in loader:\n",
        "            images, labels, _, _ ,_ = data\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs_class = net_class(images)\n",
        "            _, predicted = torch.max(outputs_class.data, 1)\n",
        "            batch_size = outputs_class.size()[0]  # batch_size\n",
        "\n",
        "            exp_prediction = expert_fn(images, labels)\n",
        "            for i in range(0, batch_size):\n",
        "                r = (expert_k >= labels[i].item()) \n",
        "                final_pred = 0\n",
        "                #r = (exp_prediction[i] == labels[i].item()), this has noise\n",
        "                if r == 0:\n",
        "                    total += 1\n",
        "                    prediction = predicted[i]\n",
        "                    if predicted[i] == n_classes:\n",
        "                        max_idx = 0\n",
        "                        for j in range(0, n_classes):\n",
        "                            if outputs_class.data[i][j] >= outputs_class.data[i][max_idx]:\n",
        "                                max_idx = j\n",
        "                        prediction = max_idx\n",
        "                    else:\n",
        "                        prediction = predicted[i]\n",
        "                    final_pred = prediction\n",
        "                    correct += (prediction == labels[i]).item()\n",
        "                    correct_sys += (prediction == labels[i]).item()\n",
        "                if r == 1:\n",
        "                    final_pred = exp_prediction[i]\n",
        "                    exp += (exp_prediction[i] == labels[i].item())\n",
        "                    correct_sys += (exp_prediction[i] == labels[i].item())\n",
        "                    exp_total += 1\n",
        "                real_total += 1\n",
        "                if labels[i].item() == final_pred:\n",
        "                    correct_pred[cifar_classes[labels[i].item()]] += 1\n",
        "                total_pred[cifar_classes[labels[i].item()]] += 1\n",
        "    cov = str(total) + str(\" out of\") + str(real_total)\n",
        "    to_print = {\"coverage\": cov, \"system accuracy\": 100 * correct_sys / real_total,\n",
        "                \"expert accuracy\": 100 * exp / (exp_total + 0.0002),\n",
        "                \"classifier accuracy\": 100 * correct / (total + 0.0001)}\n",
        "    print(to_print)\n",
        "    for classname, correct_count in correct_pred.items():\n",
        "        accuracy = 100 * float(correct_count) / total_pred[classname]\n",
        "        print(\"Accuracy for class {:5s} is: {:.3f} %\".format(classname,\n",
        "                                                    accuracy))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L92TiTGA267d"
      },
      "outputs": [],
      "source": [
        "cifar_classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "def metrics_print_classifier(model, data_loader, defer_net = False):\n",
        "    '''\n",
        "    model: model\n",
        "    data_loader: data loader\n",
        "    defer_net: boolean to indicate if model is a deferral module (has n_classes +1 outputs)\n",
        "    '''\n",
        "    # prepare to count predictions for each class\n",
        "    correct_pred = {classname: 0 for classname in cifar_classes}\n",
        "    total_pred = {classname: 0 for classname in cifar_classes}\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    # again no gradients needed\n",
        "    with torch.no_grad():\n",
        "        for data in data_loader:\n",
        "            images, labels, _, _ ,_ = data\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predictions = torch.max(outputs.data, 1) # maybe no .data\n",
        "            if defer_net:\n",
        "                predictions_fixed = predictions\n",
        "                for i in range(len(predictions_fixed)):\n",
        "                    if predictions_fixed[i] == 10: #max class\n",
        "                        max_idx = 0\n",
        "                        # get second max\n",
        "                        for j in range(0, 10):\n",
        "                            if outputs.data[i][j] >= outputs.data[i][max_idx]:\n",
        "                                max_idx = j\n",
        "                        prediction = max_idx\n",
        "                        predictions_fixed[i] = prediction\n",
        "            total += labels.size(0)\n",
        "            correct += (predictions == labels).sum().item()\n",
        "            # collect the correct predictions for each class\n",
        "            for label, prediction in zip(labels, predictions):\n",
        "                if label == prediction:\n",
        "                    correct_pred[cifar_classes[label]] += 1\n",
        "                total_pred[cifar_classes[label]] += 1\n",
        "\n",
        "    print('Accuracy of the network on the %d test images: %.3f %%' % (len(data_loader),\n",
        "        100 * correct / total))\n",
        "    # print accuracy for each class\n",
        "    for classname, correct_count in correct_pred.items():\n",
        "        accuracy = 100 * float(correct_count) / total_pred[classname]\n",
        "        print(\"Accuracy for class {:5s} is: {:.3f} %\".format(classname,\n",
        "                                                    accuracy))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqf_r99A1OcX"
      },
      "source": [
        "# Initialize expert and model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wZxCEtfhRAFB"
      },
      "outputs": [],
      "source": [
        "k = 5 # number of classes expert can predict\n",
        "n_dataset = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qXfR2kpyzs_5"
      },
      "outputs": [],
      "source": [
        "\n",
        "class synth_expert:\n",
        "    '''\n",
        "    simple class to describe our synthetic expert on CIFAR-10\n",
        "    ----\n",
        "    k: number of classes expert can predict\n",
        "    n_classes: number of classes (10+1 for CIFAR-10)\n",
        "    '''\n",
        "    def __init__(self, k, n_classes):\n",
        "        self.k = k\n",
        "        self.n_classes = n_classes\n",
        "\n",
        "    def predict(self, input, labels):\n",
        "        batch_size = labels.size()[0]  # batch_size\n",
        "        outs = [0] * batch_size\n",
        "        for i in range(0, batch_size):\n",
        "            if labels[i].item() <= self.k -1: # CHANGE FROM OLD PAPER\n",
        "                outs[i] = labels[i].item()\n",
        "            else:\n",
        "                # change to determinsticly false\n",
        "                prediction_rand = random.randint(0, self.n_classes - 1)\n",
        "                outs[i] = prediction_rand\n",
        "        return outs\n",
        "\n",
        "\n",
        "Expert = synth_expert(k, n_dataset)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5PwaJBFx2dFA"
      },
      "outputs": [],
      "source": [
        "model = WideResNet(10, n_dataset + 1, 4, dropRate=0)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Active Learning Prep"
      ],
      "metadata": {
        "id": "uUHnGa-M-GGA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# at each point maintain, a dataset full of labeled points, dataset of unlabeled points\n",
        "# initially: random points\n",
        "# train on labaled dataset\n",
        "# compute score on unlabaled data\n",
        "# move top unlabaled points to labeled set \n",
        "# re-train"
      ],
      "metadata": {
        "id": "9TP5wuHT-IWj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "use_data_aug = False\n",
        "n_dataset = 10  # cifar-10\n",
        "normalize = transforms.Normalize(mean=[x / 255.0 for x in [125.3, 123.0, 113.9]],\n",
        "                                    std=[x / 255.0 for x in [63.0, 62.1, 66.7]])\n",
        "\n",
        "if use_data_aug:\n",
        "    transform_train = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Lambda(lambda x: F.pad(x.unsqueeze(0),\n",
        "                                            (4, 4, 4, 4), mode='reflect').squeeze()),\n",
        "        transforms.ToPILImage(),\n",
        "        transforms.RandomCrop(32),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        normalize,\n",
        "    ])\n",
        "else:\n",
        "    transform_train = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        normalize,\n",
        "    ])\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    normalize\n",
        "])\n",
        "\n",
        "if n_dataset == 10:\n",
        "    dataset = 'cifar10'\n",
        "elif n_dataset == 100:\n",
        "    dataset = 'cifar100'\n",
        "\n",
        "kwargs = {'num_workers': 0, 'pin_memory': True}\n",
        "\n",
        "\n",
        "train_dataset_all = datasets.__dict__[dataset.upper()]('../data', train=True, download=True,\n",
        "                                                        transform=transform_train)\n",
        "train_size = int(0.90 * len(train_dataset_all))\n",
        "test_size = len(train_dataset_all) - train_size\n",
        "\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(train_dataset_all, [train_size, test_size])\n",
        "#train_loader = torch.utils.data.DataLoader(train_dataset,\n",
        "#                                           batch_size=128, shuffle=True, **kwargs)\n",
        "#val_loader = torch.utils.data.DataLoader(val_dataset,\n",
        "#                                            batch_size=128, shuffle=True, **kwargs)\n",
        "\n",
        "\n",
        "normalize = transforms.Normalize(mean=[x / 255.0 for x in [125.3, 123.0, 113.9]],\n",
        "                                 std=[x / 255.0 for x in [63.0, 62.1, 66.7]])\n",
        "kwargs = {'num_workers': 1, 'pin_memory': True}\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    normalize\n",
        "])\n",
        "test_dataset = datasets.__dict__[\"cifar10\".upper()]('../data', train=False, transform=transform_test, download=True)\n",
        "#test_loader = torch.utils.data.DataLoader(\n",
        "#    datasets.__dict__[\"cifar100\".upper()]('../data', train=False, transform=transform_test, download=True),\n",
        "#    batch_size=128, shuffle=True, **kwargs)\n",
        "\n"
      ],
      "metadata": {
        "id": "hxukz8OwMX7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CifarExpertDataset(Dataset):\n",
        "    def __init__(self, images, targets, expert_fn, labeled, indices = None, expert_preds = None):\n",
        "        \"\"\"\n",
        "        \"\"\"\n",
        "        self.images = images\n",
        "        self.targets = np.array(targets)\n",
        "        self.expert_fn = expert_fn\n",
        "        self.labeled = np.array(labeled)\n",
        "        if expert_preds is not None:\n",
        "            self.expert_preds = expert_preds\n",
        "        else:\n",
        "            self.expert_preds = np.array(expert_fn(None, torch.FloatTensor(targets)))\n",
        "        for i in range(len(self.expert_preds)):\n",
        "            if self.labeled[i] == 0:\n",
        "                self.expert_preds[i] = -1 # not labeled by expert\n",
        "        if indices is not None:\n",
        "            self.indices = indices\n",
        "        else:\n",
        "            self.indices = np.array(list(range(len(self.targets))))\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"Take the index of item and returns the image, label, expert prediction and index in original dataset\"\"\"\n",
        "        label = self.targets[index]\n",
        "        image = transform_test(self.images[index])\n",
        "        expert_pred = self.expert_preds[index]\n",
        "        indice = self.indices[index]\n",
        "        labeled = self.labeled[index]\n",
        "        return torch.FloatTensor(image), label, expert_pred, indice, labeled\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.targets)"
      ],
      "metadata": {
        "id": "hNzQOWWtC-Qg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_train = CifarExpertDataset(np.array(train_dataset.dataset.data)[train_dataset.indices], np.array(train_dataset.dataset.targets)[train_dataset.indices], Expert.predict , [1]*len(train_dataset.indices))\n",
        "dataset_val = CifarExpertDataset(np.array(val_dataset.dataset.data)[val_dataset.indices], np.array(val_dataset.dataset.targets)[val_dataset.indices], Expert.predict , [1]*len(val_dataset.indices))\n",
        "dataset_test = CifarExpertDataset(test_dataset.data , test_dataset.targets, Expert.predict , [1]*len(test_dataset.targets))\n",
        "\n",
        "dataLoaderTrain = DataLoader(dataset=dataset_train, batch_size=128, shuffle=True,  num_workers=0, pin_memory=True)\n",
        "dataLoaderVal = DataLoader(dataset=dataset_val, batch_size=128, shuffle=False,  num_workers=0, pin_memory=True)\n",
        "dataLoaderTest = DataLoader(dataset=dataset_test, batch_size=128, shuffle=False,  num_workers=0, pin_memory=True)"
      ],
      "metadata": {
        "id": "KWIHchCVWMP9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_reject(train_loader, model, optimizer, scheduler, epoch, expert_fn, n_classes, alpha):\n",
        "    \"\"\"Train for one epoch on the training set with deferral\"\"\"\n",
        "    batch_time = AverageMeter()\n",
        "    losses = AverageMeter()\n",
        "    top1 = AverageMeter()\n",
        "\n",
        "    # switch to train mode\n",
        "    model.train()\n",
        "\n",
        "    end = time.time()\n",
        "    for i, (input, target, expert, _, _ ) in enumerate(train_loader):\n",
        "        target = target.to(device)\n",
        "        input = input.to(device)\n",
        "        m = expert.to(device)\n",
        "        # compute output\n",
        "        output = model(input)\n",
        "\n",
        "        # get expert  predictions and costs\n",
        "        batch_size = output.size()[0]  # batch_size\n",
        "        m2 = [0] * batch_size\n",
        "        for j in range(0, batch_size):\n",
        "            if m[j].item() == target[j].item():\n",
        "                m[j] = 1\n",
        "                m2[j] = alpha\n",
        "            else:\n",
        "                m[j] = 0\n",
        "                m2[j] = 1\n",
        "        m = torch.tensor(m)\n",
        "        m2 = torch.tensor(m2)\n",
        "        m = m.to(device)\n",
        "        m2 = m2.to(device)\n",
        "        # done getting expert predictions and costs \n",
        "        # compute loss\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        loss = reject_CrossEntropyLoss(output, m, target, m2, n_classes)\n",
        "\n",
        "        # measure accuracy and record loss\n",
        "        prec1 = accuracy(output.data, target, topk=(1,))[0]\n",
        "        losses.update(loss.data.item(), input.size(0))\n",
        "        top1.update(prec1.item(), input.size(0))\n",
        "\n",
        "        # compute gradient and do SGD step\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        # measure elapsed time\n",
        "        batch_time.update(time.time() - end)\n",
        "        end = time.time()\n",
        "\n",
        "        if i % 10 == 0:\n",
        "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
        "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
        "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
        "                  'Prec@1 {top1.val:.3f} ({top1.avg:.3f})'.format(\n",
        "                epoch, i, len(train_loader), batch_time=batch_time,\n",
        "                loss=losses, top1=top1))\n",
        "\n",
        "\n",
        "def train_reject_pseudo(train_loader, model, optimizer, scheduler, epoch, expert_fn, n_classes, alpha):\n",
        "    \"\"\"Train for one epoch on the training set with deferral\"\"\"\n",
        "    batch_time = AverageMeter()\n",
        "    losses = AverageMeter()\n",
        "    top1 = AverageMeter()\n",
        "\n",
        "    # switch to train mode\n",
        "    model.train()\n",
        "\n",
        "    end = time.time()\n",
        "    for i, (input, target, expert, _, _ ) in enumerate(train_loader):\n",
        "        target = target.to(device)\n",
        "        input = input.to(device)\n",
        "        m = expert.to(device)\n",
        "        # compute output\n",
        "        output = model(input)\n",
        "\n",
        "        # get expert  predictions and costs\n",
        "        batch_size = output.size()[0]  # batch_size\n",
        "        m2 = [1] * batch_size\n",
        "\n",
        "        m = torch.tensor(m)\n",
        "        m2 = torch.tensor(m2)\n",
        "        m = m.to(device)\n",
        "        m2 = m2.to(device)\n",
        "        # done getting expert predictions and costs \n",
        "        # compute loss\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        loss = reject_CrossEntropyLoss(output, m, target, m2, n_classes)\n",
        "\n",
        "        # measure accuracy and record loss\n",
        "        prec1 = accuracy(output.data, target, topk=(1,))[0]\n",
        "        losses.update(loss.data.item(), input.size(0))\n",
        "        top1.update(prec1.item(), input.size(0))\n",
        "\n",
        "        # compute gradient and do SGD step\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        # measure elapsed time\n",
        "        batch_time.update(time.time() - end)\n",
        "        end = time.time()\n",
        "\n",
        "        if i % 10 == 0:\n",
        "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
        "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
        "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
        "                  'Prec@1 {top1.val:.3f} ({top1.avg:.3f})'.format(\n",
        "                epoch, i, len(train_loader), batch_time=batch_time,\n",
        "                loss=losses, top1=top1))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def train_reject_class(train_loader, model, optimizer, scheduler, epoch):\n",
        "    \"\"\"Train for one epoch on the training set without deferral\"\"\"\n",
        "    batch_time = AverageMeter()\n",
        "    losses = AverageMeter()\n",
        "    top1 = AverageMeter()\n",
        "\n",
        "    # switch to train mode\n",
        "    model.train()\n",
        "\n",
        "    end = time.time()\n",
        "    for i, (input, target, expert, _, _ ) in enumerate(train_loader):\n",
        "        target = target.to(device)\n",
        "        input = input.to(device)\n",
        "        # compute output\n",
        "        output = model(input)\n",
        "\n",
        "        # compute loss\n",
        "        loss = my_CrossEntropyLoss(output, target)\n",
        "\n",
        "        # measure accuracy and record loss\n",
        "        prec1 = accuracy(output.data, target, topk=(1,))[0]\n",
        "        losses.update(loss.data.item(), input.size(0))\n",
        "        top1.update(prec1.item(), input.size(0))\n",
        "\n",
        "        # compute gradient and do SGD step\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        # measure elapsed time\n",
        "        batch_time.update(time.time() - end)\n",
        "        end = time.time()\n",
        "\n",
        "        if i % 10 == 0:\n",
        "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
        "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
        "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
        "                  'Prec@1 {top1.val:.3f} ({top1.avg:.3f})'.format(\n",
        "                epoch, i, len(train_loader), batch_time=batch_time,\n",
        "                loss=losses, top1=top1))\n",
        "\n",
        "\n",
        "def validate_reject(val_loader, model, epoch, expert_fn, n_classes):\n",
        "    \"\"\"Perform validation on the validation set with deferral\"\"\"\n",
        "    batch_time = AverageMeter()\n",
        "    losses = AverageMeter()\n",
        "    top1 = AverageMeter()\n",
        "\n",
        "    # switch to evaluate mode\n",
        "    model.eval()\n",
        "\n",
        "    end = time.time()\n",
        "    for i, (input, target, expert, _ , _ ) in enumerate(val_loader):\n",
        "        target = target.to(device)\n",
        "        input = input.to(device)\n",
        "\n",
        "        # compute output\n",
        "        with torch.no_grad():\n",
        "            output = model(input)\n",
        "        # expert prediction\n",
        "        batch_size = output.size()[0]  # batch_size\n",
        "        m = expert\n",
        "        alpha = 1\n",
        "        m2 = [0] * batch_size\n",
        "        for j in range(0, batch_size):\n",
        "            if m[j] == target[j].item():\n",
        "                m[j] = 1\n",
        "                m2[j] = alpha\n",
        "            else:\n",
        "                m[j] = 0\n",
        "                m2[j] = 1\n",
        "        m = torch.tensor(m)\n",
        "        m2 = torch.tensor(m2)\n",
        "        m = m.to(device)\n",
        "        m2 = m2.to(device)\n",
        "        # compute loss\n",
        "        loss = reject_CrossEntropyLoss(output, m, target, m2, n_classes)\n",
        "\n",
        "        # measure accuracy and record loss\n",
        "        prec1 = accuracy(output.data, target, topk=(1,))[0]\n",
        "        losses.update(loss.data.item(), input.size(0))\n",
        "        top1.update(prec1.item(), input.size(0))\n",
        "\n",
        "        # measure elapsed time\n",
        "        batch_time.update(time.time() - end)\n",
        "        end = time.time()\n",
        "\n",
        "        if i % 10 == 0:\n",
        "            print('Test: [{0}/{1}]\\t'\n",
        "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
        "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
        "                  'Prec@1 {top1.val:.3f} ({top1.avg:.3f})'.format(\n",
        "                i, len(val_loader), batch_time=batch_time, loss=losses,\n",
        "                top1=top1))\n",
        "\n",
        "    print(' * Prec@1 {top1.avg:.3f}'.format(top1=top1))\n",
        "\n",
        "    return top1.avg\n"
      ],
      "metadata": {
        "id": "oDcPb90Bc4xT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_reject_pseudo(model, n_dataset, expert_fn, epochs, alpha, train_loader, val_loader, best_on_val = False, epoch_freq = 10):\n",
        "    '''\n",
        "    model: WideResNet model\n",
        "    data_aug: boolean to use data augmentation in training\n",
        "    n_dataset: number of classes\n",
        "    expert_fn: expert model\n",
        "    epochs: number of epochs to train\n",
        "    alpha: alpha parameter in L_{CE}^{\\alpha}\n",
        "    '''\n",
        "    # Data loading code\n",
        "   \n",
        "    # get the number of model parameters\n",
        "    print('Number of model parameters: {}'.format(\n",
        "        sum([p.data.nelement() for p in model.parameters()])))\n",
        "\n",
        "    # for training on multiple GPUs.\n",
        "    # Use CUDA_VISIBLE_DEVICES=0,1 to specify which GPUs to use\n",
        "    # model = torch.nn.DataParallel(model).cuda()\n",
        "    model = model.to(device)\n",
        "\n",
        "    # optionally resume from a checkpoint\n",
        "\n",
        "    cudnn.benchmark = True\n",
        "\n",
        "    # define loss function (criterion) and optimizer\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01, amsgrad=False)\n",
        "\n",
        "    # cosine learning rate\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, len(train_loader) * epochs)\n",
        "    \n",
        "    best_model = copy.deepcopy(model.state_dict())\n",
        "    best_val_score = 0\n",
        "    for epoch in range(0, epochs):\n",
        "        # train for one epoch\n",
        "        train_reject_pseudo(train_loader, model, optimizer, scheduler, epoch, expert_fn, n_dataset, alpha)\n",
        "        if epoch % epoch_freq == 0:\n",
        "            score = metrics_print(model, expert_fn, n_dataset, val_loader)['system accuracy']\n",
        "            if score > best_val_score:\n",
        "                best_model = copy.deepcopy(model.state_dict())\n",
        "    if best_on_val:\n",
        "        return  best_model \n"
      ],
      "metadata": {
        "id": "l69fcTT_EPEe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def run_reject(model, n_dataset, expert_fn, epochs, alpha, train_loader, val_loader, best_on_val = False, epoch_freq = 10):\n",
        "    '''\n",
        "    model: WideResNet model\n",
        "    data_aug: boolean to use data augmentation in training\n",
        "    n_dataset: number of classes\n",
        "    expert_fn: expert model\n",
        "    epochs: number of epochs to train\n",
        "    alpha: alpha parameter in L_{CE}^{\\alpha}\n",
        "    '''\n",
        "    # Data loading code\n",
        "   \n",
        "    # get the number of model parameters\n",
        "    print('Number of model parameters: {}'.format(\n",
        "        sum([p.data.nelement() for p in model.parameters()])))\n",
        "\n",
        "    # for training on multiple GPUs.\n",
        "    # Use CUDA_VISIBLE_DEVICES=0,1 to specify which GPUs to use\n",
        "    # model = torch.nn.DataParallel(model).cuda()\n",
        "    model = model.to(device)\n",
        "\n",
        "    # optionally resume from a checkpoint\n",
        "\n",
        "    cudnn.benchmark = True\n",
        "\n",
        "    # define loss function (criterion) and optimizer\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01, amsgrad=False)\n",
        "\n",
        "    # cosine learning rate\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, len(train_loader) * epochs)\n",
        "    \n",
        "    best_model = copy.deepcopy(model.state_dict())\n",
        "    best_val_score = 0\n",
        "    for epoch in range(0, epochs):\n",
        "        # train for one epoch\n",
        "        train_reject(train_loader, model, optimizer, scheduler, epoch, expert_fn, n_dataset, alpha)\n",
        "        if epoch % epoch_freq == 0:\n",
        "            score = metrics_print(model, expert_fn, n_dataset, val_loader)['system accuracy']\n",
        "            if score > best_val_score:\n",
        "                best_model = copy.deepcopy(model.state_dict())\n",
        "    if best_on_val:\n",
        "        return  best_model \n",
        "\n",
        "\n",
        "def run_reject_class(model, epochs, train_loader, val_loader):\n",
        "    '''\n",
        "    only train classifier\n",
        "    model: WideResNet model\n",
        "    epochs: number of epochs to train\n",
        "    '''\n",
        "    # get the number of model parameters\n",
        "    print('Number of model parameters: {}'.format(\n",
        "        sum([p.data.nelement() for p in model.parameters()])))\n",
        "\n",
        "    # define loss function (criterion) and optimizer\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01, amsgrad=False)\n",
        "\n",
        "\n",
        "    # cosine learning rate\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, len(train_loader) * epochs)\n",
        "\n",
        "    for epoch in range(0, epochs):\n",
        "        # train for one epoch\n",
        "        train_reject_class(train_loader, model, optimizer, scheduler, epoch)\n",
        "        if epoch % 10 == 0:\n",
        "            metrics_print_classifier(model, val_loader)\n",
        "\n"
      ],
      "metadata": {
        "id": "uSuMhHTGdGg-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = NetSimple(n_dataset + 1, 100, 100, 1000,500).to(device)"
      ],
      "metadata": {
        "id": "50GC4YyWG8D0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#run_reject_class(model, 100, dataLoaderTrain, dataLoaderVal)"
      ],
      "metadata": {
        "id": "Usl2MBKbHAxI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Optimal Solution"
      ],
      "metadata": {
        "id": "s8bHnx2qbN2o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = NetSimple(n_dataset + 1, 100, 100, 1000,500).to(device)"
      ],
      "metadata": {
        "id": "OuXS4uU7bRHC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_reject(model, 10, Expert.predict, 70,0.5, dataLoaderTrain, dataLoaderVal)\n"
      ],
      "metadata": {
        "id": "290EKr-kbWAR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Active Learning"
      ],
      "metadata": {
        "id": "Zot1njxRujwn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Algorithm 1\n",
        "\n",
        "Example gathering: use uncertainty on expert\n",
        "model training: post-hoc rejector calibrated"
      ],
      "metadata": {
        "id": "aUdzMKjfvSbe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#model = NetSimple(n_dataset + 1, 100, 100, 1000,500).to(device)\n",
        "run_reject_class(model, 10, dataLoaderTrain, dataLoaderVal)"
      ],
      "metadata": {
        "id": "nBiX8F4sR1i1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics_print_classifier(model, dataLoaderVal)"
      ],
      "metadata": {
        "id": "rkoMqa8o20mq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Active Learning parameters\n",
        "INITIAL_SIZE = 100\n",
        "BATCH_SIZE_AL = 100\n",
        "MAX_ROUNDS = 10\n",
        "EPOCH_TRAIN = 10\n",
        "MAX_TRIALS = 1"
      ],
      "metadata": {
        "id": "DP7UruLjr2mQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_indices = list(range(len(train_dataset.indices)))\n",
        "all_data_x = np.array(train_dataset.dataset.data)[train_dataset.indices]\n",
        "all_data_y = np.array(train_dataset.dataset.targets)[train_dataset.indices]\n",
        "\n",
        "intial_random_set = random.sample(all_indices, INITIAL_SIZE)\n",
        "indices_labeled  = intial_random_set\n",
        "indices_unlabeled= list(set(all_indices) - set(indices_labeled))\n",
        "\n",
        "dataset_train_labeled = CifarExpertDataset(all_data_x[indices_labeled], all_data_y[indices_labeled], Expert.predict , [1]*len(indices_labeled), indices_labeled)\n",
        "dataset_train_unlabeled = CifarExpertDataset(all_data_x[indices_unlabeled], all_data_y[indices_unlabeled], Expert.predict , [0]*len(indices_unlabeled), indices_unlabeled)\n",
        "\n",
        "dataLoaderTrainLabeled = DataLoader(dataset=dataset_train_labeled, batch_size=128, shuffle=True,  num_workers=0, pin_memory=True)\n",
        "dataLoaderTrainUnlabeled = DataLoader(dataset=dataset_train_unlabeled, batch_size=128, shuffle=True,  num_workers=0, pin_memory=True)"
      ],
      "metadata": {
        "id": "H1nvi-aKpREl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_expert_confidence(train_loader, model, optimizer, scheduler, epoch):\n",
        "    \"\"\"Train for one epoch on the training set without deferral\"\"\"\n",
        "    batch_time = AverageMeter()\n",
        "    losses = AverageMeter()\n",
        "    top1 = AverageMeter()\n",
        "\n",
        "    # switch to train mode\n",
        "    model.train()\n",
        "\n",
        "    end = time.time()\n",
        "    for i, (input, label, expert_pred, _, _ ) in enumerate(train_loader):\n",
        "        expert_pred = expert_pred.long()\n",
        "        expert_pred = (expert_pred == label) *1\n",
        "        target = expert_pred.to(device)\n",
        "        input = input.to(device)\n",
        "        # compute output\n",
        "        output = model(input)\n",
        "\n",
        "        # compute loss\n",
        "        loss = my_CrossEntropyLoss(output, target)\n",
        "\n",
        "        # measure accuracy and record loss\n",
        "        prec1 = accuracy(output.data, target, topk=(1,))[0]\n",
        "        losses.update(loss.data.item(), input.size(0))\n",
        "        top1.update(prec1.item(), input.size(0))\n",
        "\n",
        "        # compute gradient and do SGD step\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        # measure elapsed time\n",
        "        batch_time.update(time.time() - end)\n",
        "        end = time.time()\n",
        "\n",
        "        if i % 10 == 0:\n",
        "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
        "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
        "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
        "                  'Prec@1 {top1.val:.3f} ({top1.avg:.3f})'.format(\n",
        "                epoch, i, len(train_loader), batch_time=batch_time,\n",
        "                loss=losses, top1=top1))\n",
        "            \n",
        "\n",
        "\n",
        "def metrics_print_expert(model, data_loader, defer_net = False):\n",
        "    '''\n",
        "    model: model\n",
        "    data_loader: data loader\n",
        "    '''\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    # again no gradients needed\n",
        "    with torch.no_grad():\n",
        "        for data in data_loader:\n",
        "            images, label, expert_pred, _ ,_ = data\n",
        "            expert_pred = expert_pred.long()\n",
        "            expert_pred = (expert_pred == label) *1\n",
        "            images, labels = images.to(device), expert_pred.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predictions = torch.max(outputs.data, 1) # maybe no .data\n",
        "\n",
        "            total += labels.size(0)\n",
        "            correct += (predictions == labels).sum().item()\n",
        "\n",
        "    print('Accuracy of the network on the %d test images: %.3f %%' % (total,\n",
        "        100 * correct / total))\n",
        "\n",
        "def run_expert(model, epochs, train_loader, val_loader):\n",
        "    '''\n",
        "    only train classifier\n",
        "    model: WideResNet model\n",
        "    epochs: number of epochs to train\n",
        "    '''\n",
        "    # get the number of model parameters\n",
        "    print('Number of model parameters: {}'.format(\n",
        "        sum([p.data.nelement() for p in model.parameters()])))\n",
        "\n",
        "    # define loss function (criterion) and optimizer\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01, amsgrad=False)\n",
        "\n",
        "\n",
        "    # cosine learning rate\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, len(train_loader) * epochs)\n",
        "\n",
        "    for epoch in range(0, epochs):\n",
        "        # train for one epoch\n",
        "        train_expert_confidence(train_loader, model, optimizer, scheduler, epoch)\n",
        "        if epoch % 10 == 0:\n",
        "            metrics_print_expert(model, val_loader)\n",
        "    metrics_print_expert(model, val_loader)"
      ],
      "metadata": {
        "id": "kVY0AgBCwJHd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def metrics_print_2step(net_mod, net_exp, expert_fn, n_classes, loader):\n",
        "    correct = 0\n",
        "    correct_sys = 0\n",
        "    exp = 0\n",
        "    exp_total = 0\n",
        "    total = 0\n",
        "    real_total = 0\n",
        "    with torch.no_grad():\n",
        "        for data in loader:\n",
        "            images, labels, expert_preds, _, _ = data\n",
        "            images, labels, expert_preds = images.to(device), labels.to(device), expert_preds.to(device)\n",
        "            outputs_mod = net_mod(images)\n",
        "            outputs_exp = net_exp(images)\n",
        "            _, predicted = torch.max(outputs_mod.data, 1)\n",
        "            _, predicted_exp = torch.max(outputs_exp.data, 1)\n",
        "            batch_size = outputs_mod.size()[0]  # batch_size\n",
        "            exp_prediction = expert_fn(images, labels)\n",
        "            for i in range(0, batch_size):\n",
        "                r_score =  outputs_mod.data[i][predicted[i].item()].item()\n",
        "                r_score = outputs_exp.data[i][1].item() - r_score\n",
        "                r = 0\n",
        "                if r_score >= 0:\n",
        "                    r = 1\n",
        "                else:\n",
        "                    r = 0\n",
        "                if r == 0:\n",
        "                    total += 1\n",
        "                    correct += (predicted[i] == labels[i]).item()\n",
        "                    correct_sys += (predicted[i] == labels[i]).item()\n",
        "                if r == 1:\n",
        "                    exp += (exp_prediction[i] == labels[i].item())\n",
        "                    correct_sys += (exp_prediction[i] == labels[i].item())\n",
        "                    exp_total += 1\n",
        "                real_total += 1\n",
        "    cov = str(total) + str(\" out of\") + str(real_total)\n",
        "    to_print = {\"coverage\": cov, \"system accuracy\": 100 * correct_sys / real_total,\n",
        "                \"expert accuracy\": 100 * exp / (exp_total + 0.0002),\n",
        "                \"classifier accuracy\": 100 * correct / (total + 0.0001)}\n",
        "    return to_print\n",
        "    print(to_print)"
      ],
      "metadata": {
        "id": "w7ypH-FOOpPB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Intial_random_set = random.sample(all_indices, INITIAL_SIZE)\n"
      ],
      "metadata": {
        "id": "oqo7eUnW5CHU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# random selection\n",
        "error_random_trials = []\n",
        "for trial in range(MAX_TRIALS):\n",
        "    print(f'\\n \\n \\n Trial {trial} \\n \\n \\n ')\n",
        "    all_indices = list(range(len(train_dataset.indices)))\n",
        "    all_data_x = np.array(train_dataset.dataset.data)[train_dataset.indices]\n",
        "    all_data_y = np.array(train_dataset.dataset.targets)[train_dataset.indices]\n",
        "\n",
        "    indices_labeled  = Intial_random_set\n",
        "    indices_unlabeled= list(set(all_indices) - set(indices_labeled))\n",
        "\n",
        "    dataset_train_labeled = CifarExpertDataset(all_data_x[indices_labeled], all_data_y[indices_labeled], Expert.predict , [1]*len(indices_labeled), indices_labeled)\n",
        "    dataset_train_unlabeled = CifarExpertDataset(all_data_x[indices_unlabeled], all_data_y[indices_unlabeled], Expert.predict , [0]*len(indices_unlabeled), indices_unlabeled)\n",
        "\n",
        "    dataLoaderTrainLabeled = DataLoader(dataset=dataset_train_labeled, batch_size=128, shuffle=True,  num_workers=0, pin_memory=True)\n",
        "    dataLoaderTrainUnlabeled = DataLoader(dataset=dataset_train_unlabeled, batch_size=128, shuffle=True,  num_workers=0, pin_memory=True)\n",
        "\n",
        "\n",
        "    model_expert = NetSimple(2, 100, 100, 1000,500).to(device)\n",
        "    run_expert(model_expert, EPOCH_TRAIN, dataLoaderTrainLabeled, dataLoaderTrainLabeled)\n",
        "    data_sizes = []\n",
        "    error_random = []\n",
        "\n",
        "    data_sizes.append(INITIAL_SIZE)\n",
        "    metrics_random = metrics_print_2step(model, model_expert, Expert.predict, 10, dataLoaderVal)\n",
        "    error_random.append(metrics_random['system accuracy'])\n",
        "\n",
        "\n",
        "    for round in range(MAX_ROUNDS):\n",
        "        print(f'\\n \\n Round {round} \\n \\n')\n",
        "        intial_random_set = random.sample(indices_unlabeled, BATCH_SIZE_AL)\n",
        "\n",
        "        indices_labeled  = indices_labeled + intial_random_set \n",
        "        indices_unlabeled= list(set(all_indices) - set(indices_labeled))\n",
        "        dataset_train_labeled = CifarExpertDataset(all_data_x[indices_labeled], all_data_y[indices_labeled], Expert.predict , [1]*len(indices_labeled))\n",
        "        dataset_train_unlabeled = CifarExpertDataset(all_data_x[indices_unlabeled], all_data_y[indices_unlabeled], Expert.predict , [0]*len(indices_unlabeled))\n",
        "        dataLoaderTrainLabeled = DataLoader(dataset=dataset_train_labeled, batch_size=128, shuffle=True,  num_workers=0, pin_memory=True)\n",
        "        dataLoaderTrainUnlabeled = DataLoader(dataset=dataset_train_unlabeled, batch_size=128, shuffle=True,  num_workers=0, pin_memory=True)\n",
        "        dataLoaderTrainUnlabeledUnshuffled = DataLoader(dataset=dataset_train_unlabeled, batch_size=128, shuffle=False,  num_workers=0, pin_memory=True)\n",
        "\n",
        "\n",
        "        run_expert(model_expert, EPOCH_TRAIN, dataLoaderTrainLabeled, dataLoaderTrainLabeled)\n",
        "\n",
        "        metrics_random = metrics_print_2step(model, model_expert, Expert.predict, 10, dataLoaderTest)\n",
        "        error_random.append(metrics_random['system accuracy'])\n",
        "        data_sizes.append((round+1)*BATCH_SIZE_AL + INITIAL_SIZE)\n",
        "    error_random_trials.append(error_random)"
      ],
      "metadata": {
        "id": "mj4i8jVNZuWE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# confidence selection WITH LCE\n",
        "from scipy.stats import entropy\n",
        "\n",
        "def get_least_confident_points(model, data_loader, budget):\n",
        "    '''\n",
        "    based on entropy score, can chagnge, but make sure to get max or min accordingly\n",
        "    '''\n",
        "    uncertainty_estimates = []\n",
        "    indices_all = []\n",
        "    for data in data_loader:\n",
        "        images, labels, expert_preds, indices, _ = data\n",
        "        images, labels, expert_preds = images.to(device), labels.to(device), expert_preds.to(device)\n",
        "        outputs = model(images)\n",
        "        batch_size = outputs.size()[0]  \n",
        "        for i in range(0, batch_size):\n",
        "            output_i =  outputs.data[i].cpu().numpy()\n",
        "            entropy_i = entropy(output_i)\n",
        "            #entropy_i = 1 - max(output_i)\n",
        "            uncertainty_estimates.append(entropy_i)\n",
        "            indices_all.append(indices[i].item())\n",
        "    indices_all = np.array(indices_all)\n",
        "    top_budget_indices = np.argsort(uncertainty_estimates)[-budget:]\n",
        "    actual_indices = indices_all[top_budget_indices]\n",
        "    uncertainty_estimates = np.array(uncertainty_estimates)\n",
        "    return actual_indices\n",
        "import copy\n",
        "EPOCHS_DEFER = 10\n",
        "error_confidence_trials_LCE = []\n",
        "for trial in range(MAX_TRIALS):\n",
        "    print(f'\\n \\n \\n Trial {trial} \\n \\n \\n ')\n",
        "\n",
        "    all_indices = list(range(len(train_dataset.indices)))\n",
        "    all_data_x = np.array(train_dataset.dataset.data)[train_dataset.indices]\n",
        "    all_data_y = np.array(train_dataset.dataset.targets)[train_dataset.indices]\n",
        "\n",
        "    indices_labeled  = Intial_random_set\n",
        "    indices_unlabeled= list(set(all_indices) - set(indices_labeled))\n",
        "\n",
        "    dataset_train_labeled = CifarExpertDataset(all_data_x[indices_labeled], all_data_y[indices_labeled], Expert.predict , [1]*len(indices_labeled), indices_labeled)\n",
        "    dataset_train_unlabeled = CifarExpertDataset(all_data_x[indices_unlabeled], all_data_y[indices_unlabeled], Expert.predict , [0]*len(indices_unlabeled), indices_unlabeled)\n",
        "\n",
        "    dataLoaderTrainLabeled = DataLoader(dataset=dataset_train_labeled, batch_size=128, shuffle=True,  num_workers=0, pin_memory=True)\n",
        "    dataLoaderTrainUnlabeled = DataLoader(dataset=dataset_train_unlabeled, batch_size=128, shuffle=True,  num_workers=0, pin_memory=True)\n",
        "\n",
        "    model_expert = NetSimple(2, 100, 100, 1000,500).to(device)\n",
        "    run_expert(model_expert, EPOCH_TRAIN, dataLoaderTrainLabeled, dataLoaderTrainLabeled) \n",
        "\n",
        "    data_sizes = []\n",
        "    error_confidence = []\n",
        "    data_sizes.append(INITIAL_SIZE)\n",
        "\n",
        "    model_lce = NetSimple(n_dataset + 1, 100, 100, 1000,500).to(device)\n",
        "    run_reject_class(model_lce, EPOCH_TRAIN, dataLoaderTrain, dataLoaderVal)\n",
        "    model_lce_saved = copy.deepcopy(model_lce.state_dict())\n",
        "\n",
        "    dataLoaderTrainUnlabeledUnshuffled = DataLoader(dataset=dataset_train_unlabeled, batch_size=128, shuffle=False,  num_workers=0, pin_memory=True)\n",
        "    expert_preds_arr = []\n",
        "    for data in dataLoaderTrainUnlabeledUnshuffled:\n",
        "        images, labels, _, _, _ = data\n",
        "        images = images.to(device)\n",
        "        outputs_exp = model_expert(images)\n",
        "        for i in range(outputs_exp.size()[0]):\n",
        "            #pred_exp = np.argmax(outputs_exp.data[i].cpu().numpy())\n",
        "            pred_exp = outputs_exp.data[i].cpu().numpy()\n",
        "            pred_exp = pred_exp[1]\n",
        "            expert_preds_arr.append(pred_exp)\n",
        "    expert_preds_unlabeled = np.array(expert_preds_arr)\n",
        "    expert_preds_labeled = np.array(Expert.predict (None, torch.FloatTensor(all_data_y[indices_labeled])))\n",
        "    expert_preds_labeled = ( expert_preds_labeled == all_data_y[indices_labeled]) * 1\n",
        "    expert_preds_combined = np.concatenate(( expert_preds_labeled, expert_preds_unlabeled))\n",
        "    # create pseudo-labeled dataset\n",
        "    \n",
        "    dataset_train_pseudolabeled = CifarExpertDataset(np.concatenate((all_data_x[indices_labeled] , all_data_x[indices_unlabeled])),\n",
        "                                                        np.concatenate((all_data_y[indices_labeled] , all_data_y[indices_unlabeled])), Expert.predict , [1]*(len(indices_labeled) + len(indices_unlabeled)), None,\n",
        "                                                        expert_preds_combined)\n",
        "    dataLoaderTrainPseudoLabeled = DataLoader(dataset=dataset_train_pseudolabeled, batch_size=128, shuffle=True,  num_workers=0, pin_memory=True)\n",
        "\n",
        "\n",
        "    run_reject_pseudo(model_lce, 10, Expert.predict, EPOCHS_DEFER, 1, dataLoaderTrainPseudoLabeled, dataLoaderTrainLabeled)\n",
        "    metrics_confidence = metrics_print(model_lce, Expert.predict, n_dataset, dataLoaderTest)\n",
        "    error_confidence.append(metrics_confidence['system accuracy'])\n",
        "    for round in range(MAX_ROUNDS):\n",
        "        model_lce.load_state_dict(model_lce_saved)\n",
        "\n",
        "        print(f'\\n \\n Round {round} \\n \\n')\n",
        "        indices_confidence =  random.sample(indices_unlabeled, BATCH_SIZE_AL)#get_least_confident_points(model_expert, dataLoaderTrainUnlabeled, BATCH_SIZE_AL)\n",
        "        indices_labeled  = indices_labeled + list(indices_confidence) \n",
        "        indices_unlabeled= list(set(all_indices) - set(indices_labeled))\n",
        "        dataset_train_labeled = CifarExpertDataset(all_data_x[indices_labeled], all_data_y[indices_labeled], Expert.predict , [1]*len(indices_labeled))\n",
        "        dataset_train_unlabeled = CifarExpertDataset(all_data_x[indices_unlabeled], all_data_y[indices_unlabeled], Expert.predict , [0]*len(indices_unlabeled))\n",
        "        dataLoaderTrainLabeled = DataLoader(dataset=dataset_train_labeled, batch_size=128, shuffle=True,  num_workers=0, pin_memory=True)\n",
        "        dataLoaderTrainUnlabeled = DataLoader(dataset=dataset_train_unlabeled, batch_size=128, shuffle=True,  num_workers=0, pin_memory=True)\n",
        "        \n",
        "        run_expert(model_expert, EPOCH_TRAIN, dataLoaderTrainLabeled, dataLoaderTrainLabeled)\n",
        "        # get expert predictions\n",
        "        \n",
        "        dataLoaderTrainUnlabeledUnshuffled = DataLoader(dataset=dataset_train_unlabeled, batch_size=128, shuffle=False,  num_workers=0, pin_memory=True)\n",
        "        expert_preds_arr = []\n",
        "        for data in dataLoaderTrainUnlabeledUnshuffled:\n",
        "            images, labels, _, _, _ = data\n",
        "            images = images.to(device)\n",
        "            outputs_exp = model_expert(images)\n",
        "            for i in range(outputs_exp.size()[0]):\n",
        "                #pred_exp = np.argmax(outputs_exp.data[i].cpu().numpy())\n",
        "                pred_exp = outputs_exp.data[i].cpu().numpy()\n",
        "                pred_exp = pred_exp[1]\n",
        "                expert_preds_arr.append(pred_exp)\n",
        "        expert_preds_unlabeled = np.array(expert_preds_arr)\n",
        "        expert_preds_labeled = np.array(Expert.predict (None, torch.FloatTensor(all_data_y[indices_labeled])))\n",
        "        expert_preds_labeled = ( expert_preds_labeled == all_data_y[indices_labeled]) * 1\n",
        "        expert_preds_combined = np.concatenate(( expert_preds_labeled, expert_preds_unlabeled))\n",
        "        # create pseudo-labeled dataset\n",
        "        \n",
        "        dataset_train_pseudolabeled = CifarExpertDataset(np.concatenate((all_data_x[indices_labeled] , all_data_x[indices_unlabeled])),\n",
        "                                                         np.concatenate((all_data_y[indices_labeled] , all_data_y[indices_unlabeled])), Expert.predict , [1]*(len(indices_labeled) + len(indices_unlabeled))  , None,\n",
        "                                                         expert_preds_combined)\n",
        "        dataLoaderTrainPseudoLabeled = DataLoader(dataset=dataset_train_pseudolabeled, batch_size=128, shuffle=True,  num_workers=0, pin_memory=True)\n",
        "\n",
        "\n",
        "        best_score = 0\n",
        "        best_model = None\n",
        "        for alpha in [1]:\n",
        "            print(f'alpha {alpha}')\n",
        "            model_lce.load_state_dict(model_lce_saved)\n",
        "            model_dict_alpha = run_reject_pseudo(model_lce, 10, Expert.predict, EPOCHS_DEFER, 1, dataLoaderTrainPseudoLabeled, dataLoaderTest, True, EPOCHS_DEFER-1)\n",
        "            model_lce.load_state_dict(model_dict_alpha)\n",
        "            score = metrics_print(model_lce, Expert.predict, n_dataset, dataLoaderTest)['system accuracy']\n",
        "            if score >= best_score:\n",
        "                best_score =  score\n",
        "                best_model = model_dict_alpha\n",
        "        model_lce.load_state_dict(best_model)\n",
        "\n",
        "        #run_reject(model_lce, 10, Expert.predict, EPOCHS_DEFER, 1, dataLoaderTrainPseudoLabeled, dataLoaderTrainLabeled)\n",
        "        metrics_confidence = metrics_print(model_lce, Expert.predict, n_dataset, dataLoaderTest)\n",
        "        error_confidence.append(metrics_confidence['system accuracy'])\n",
        "        data_sizes.append((round+1)*BATCH_SIZE_AL + INITIAL_SIZE)\n",
        "    error_confidence_trials_LCE.append(error_confidence)"
      ],
      "metadata": {
        "id": "po1kQ2n5F-xs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## other methods"
      ],
      "metadata": {
        "id": "gC3UNcoIBmIE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# confidence selection\n",
        "from scipy.stats import entropy\n",
        "\n",
        "\n",
        "def get_least_confident_points(model, data_loader, budget):\n",
        "    '''\n",
        "    based on entropy score, can chagnge, but make sure to get max or min accordingly\n",
        "    '''\n",
        "    uncertainty_estimates = []\n",
        "    indices_all = []\n",
        "    for data in data_loader:\n",
        "        images, labels, expert_preds, indices, _ = data\n",
        "        images, labels, expert_preds = images.to(device), labels.to(device), expert_preds.to(device)\n",
        "        outputs = model(images)\n",
        "        batch_size = outputs.size()[0]  \n",
        "        for i in range(0, batch_size):\n",
        "            output_i =  outputs.data[i].cpu().numpy()\n",
        "            entropy_i = entropy(output_i)\n",
        "            #entropy_i = 1 - max(output_i)\n",
        "            uncertainty_estimates.append(entropy_i)\n",
        "            indices_all.append(indices[i].item())\n",
        "    indices_all = np.array(indices_all)\n",
        "    top_budget_indices = np.argsort(uncertainty_estimates)[-budget:]\n",
        "    actual_indices = indices_all[top_budget_indices]\n",
        "    uncertainty_estimates = np.array(uncertainty_estimates)\n",
        "    return actual_indices\n",
        "\n",
        "\n",
        "error_confidence_trials = []\n",
        "for trial in range(MAX_TRIALS):\n",
        "    print(f'\\n \\n \\n Trial {trial} \\n \\n \\n ')\n",
        "\n",
        "    all_indices = list(range(len(train_dataset.indices)))\n",
        "    all_data_x = np.array(train_dataset.dataset.data)[train_dataset.indices]\n",
        "    all_data_y = np.array(train_dataset.dataset.targets)[train_dataset.indices]\n",
        "\n",
        "    indices_labeled  = Intial_random_set\n",
        "    indices_unlabeled= list(set(all_indices) - set(indices_labeled))\n",
        "\n",
        "    dataset_train_labeled = CifarExpertDataset(all_data_x[indices_labeled], all_data_y[indices_labeled], Expert.predict , [1]*len(indices_labeled), indices_labeled)\n",
        "    dataset_train_unlabeled = CifarExpertDataset(all_data_x[indices_unlabeled], all_data_y[indices_unlabeled], Expert.predict , [0]*len(indices_unlabeled), indices_unlabeled)\n",
        "\n",
        "    dataLoaderTrainLabeled = DataLoader(dataset=dataset_train_labeled, batch_size=128, shuffle=True,  num_workers=0, pin_memory=True)\n",
        "    dataLoaderTrainUnlabeled = DataLoader(dataset=dataset_train_unlabeled, batch_size=128, shuffle=True,  num_workers=0, pin_memory=True)\n",
        "\n",
        "    model_expert = NetSimple(2, 100, 100, 1000,500).to(device)\n",
        "    run_expert(model_expert, EPOCH_TRAIN, dataLoaderTrainLabeled, dataLoaderTrainLabeled) \n",
        "    data_sizes = []\n",
        "    error_confidence = []\n",
        "    data_sizes.append(INITIAL_SIZE)\n",
        "    metrics_confidence = metrics_print_2step(model, model_expert, Expert.predict, 10, dataLoaderVal)\n",
        "    error_confidence.append(metrics_confidence['system accuracy'])\n",
        "    for round in range(MAX_ROUNDS):\n",
        "        print(f'\\n \\n Round {round} \\n \\n')\n",
        "        indices_confidence = get_least_confident_points(model_expert, dataLoaderTrainUnlabeled, BATCH_SIZE_AL)\n",
        "        indices_labeled  = indices_labeled + list(indices_confidence) \n",
        "        indices_unlabeled= list(set(all_indices) - set(indices_labeled))\n",
        "        dataset_train_labeled = CifarExpertDataset(all_data_x[indices_labeled], all_data_y[indices_labeled], Expert.predict , [1]*len(indices_labeled))\n",
        "        dataset_train_unlabeled = CifarExpertDataset(all_data_x[indices_unlabeled], all_data_y[indices_unlabeled], Expert.predict , [0]*len(indices_unlabeled))\n",
        "        dataLoaderTrainLabeled = DataLoader(dataset=dataset_train_labeled, batch_size=128, shuffle=True,  num_workers=0, pin_memory=True)\n",
        "        dataLoaderTrainUnlabeled = DataLoader(dataset=dataset_train_unlabeled, batch_size=128, shuffle=True,  num_workers=0, pin_memory=True)\n",
        "        \n",
        "        run_expert(model_expert, EPOCH_TRAIN, dataLoaderTrainLabeled, dataLoaderTrainLabeled)\n",
        "\n",
        "        metrics_confidence = metrics_print_2step(model, model_expert, Expert.predict, 10, dataLoaderVal)\n",
        "        error_confidence.append(metrics_confidence['system accuracy'])\n",
        "        data_sizes.append((round+1)*BATCH_SIZE_AL + INITIAL_SIZE)\n",
        "    error_confidence_trials.append(error_confidence)"
      ],
      "metadata": {
        "id": "l7ve6uBzmp5y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_least_confident_points_ensemble(models, data_loader, budget):\n",
        "    '''\n",
        "    based on entropy score, can chagnge, but make sure to get max or min accordingly\n",
        "    '''\n",
        "    uncertainty_estimates = []\n",
        "    indices_all = []\n",
        "    for data in data_loader:\n",
        "        images, labels, expert_preds, indices, _ = data\n",
        "        images, labels, expert_preds = images.to(device), labels.to(device), expert_preds.to(device)\n",
        "        outputs_all = []\n",
        "        for model in models:\n",
        "            output = model(images)\n",
        "            outputs_all.append(output)\n",
        "        batch_size = outputs_all[0].size()[0]  \n",
        "        for i in range(0, batch_size):\n",
        "            outputs_np = []\n",
        "            entropies = []\n",
        "            predictions = []\n",
        "            for outputs in outputs_all:\n",
        "                output_i =  outputs.data[i].cpu().numpy()\n",
        "                outputs_np.append(output_i)\n",
        "                predictions.append(np.argmax(output_i))\n",
        "                entropies.append(entropy(output_i))\n",
        "            majority_prediction = np.round(np.average(predictions))\n",
        "            disagreement_score = sum((predictions[i] != majority_prediction)*entropies[i] for i in range(len(predictions)))\n",
        "\n",
        "            entropy_i = np.std(entropies)\n",
        "            uncertainty_estimates.append(disagreement_score)\n",
        "            indices_all.append(indices[i].item())\n",
        "            \n",
        "    indices_all = np.array(indices_all)\n",
        "    top_budget_indices = np.argsort(uncertainty_estimates)[-budget:]\n",
        "    actual_indices = indices_all[top_budget_indices]\n",
        "    uncertainty_estimates = np.array(uncertainty_estimates)\n",
        "    return actual_indices\n"
      ],
      "metadata": {
        "id": "Hwn268aTnHte"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# confidence ensemble selection\n",
        "from scipy.stats import entropy\n",
        "\n",
        "error_confidenceensemble_trials = []\n",
        "ENSEMBLE_SIZE = 10\n",
        "for trial in range(MAX_TRIALS):\n",
        "    print(f'\\n \\n \\n Trial {trial} \\n \\n \\n ')\n",
        "\n",
        "    all_indices = list(range(len(train_dataset.indices)))\n",
        "    all_data_x = np.array(train_dataset.dataset.data)[train_dataset.indices]\n",
        "    all_data_y = np.array(train_dataset.dataset.targets)[train_dataset.indices]\n",
        "\n",
        "    indices_labeled  = Intial_random_set\n",
        "    indices_unlabeled= list(set(all_indices) - set(indices_labeled))\n",
        "\n",
        "    dataset_train_labeled = CifarExpertDataset(all_data_x[indices_labeled], all_data_y[indices_labeled], Expert.predict , [1]*len(indices_labeled), indices_labeled)\n",
        "    dataset_train_unlabeled = CifarExpertDataset(all_data_x[indices_unlabeled], all_data_y[indices_unlabeled], Expert.predict , [0]*len(indices_unlabeled), indices_unlabeled)\n",
        "\n",
        "    dataLoaderTrainLabeled = DataLoader(dataset=dataset_train_labeled, batch_size=128, shuffle=True,  num_workers=0, pin_memory=True)\n",
        "    dataLoaderTrainUnlabeled = DataLoader(dataset=dataset_train_unlabeled, batch_size=128, shuffle=True,  num_workers=0, pin_memory=True)\n",
        "\n",
        "    model_expert_OG = NetSimple(2, 100, 100, 1000,500).to(device)\n",
        "    run_expert(model_expert, EPOCH_TRAIN, dataLoaderTrainLabeled, dataLoaderTrainLabeled) \n",
        "    data_sizes = []\n",
        "    error_confidence = []\n",
        "    data_sizes.append(INITIAL_SIZE)\n",
        "    metrics_confidence = metrics_print_2step(model, model_expert_OG, Expert.predict, 10, dataLoaderVal)\n",
        "    error_confidence.append(metrics_confidence['system accuracy'])\n",
        "    for round in range(MAX_ROUNDS):\n",
        "        print(f'\\n \\n Round {round} \\n \\n')\n",
        "        ensemble_experts = []\n",
        "        for ens in range(ENSEMBLE_SIZE):\n",
        "            model_expert = NetSimple(2, 100, 100, 1000,500).to(device)\n",
        "            run_expert(model_expert, EPOCH_TRAIN, dataLoaderTrainLabeled, dataLoaderTrainLabeled)\n",
        "            ensemble_experts.append(model_expert)\n",
        "        indices_confidence = get_least_confident_points_ensemble(ensemble_experts, dataLoaderTrainUnlabeled, BATCH_SIZE_AL)\n",
        "        indices_labeled  = indices_labeled + list(indices_confidence) \n",
        "        indices_unlabeled= list(set(all_indices) - set(indices_labeled))\n",
        "        dataset_train_labeled = CifarExpertDataset(all_data_x[indices_labeled], all_data_y[indices_labeled], Expert.predict , [1]*len(indices_labeled))\n",
        "        dataset_train_unlabeled = CifarExpertDataset(all_data_x[indices_unlabeled], all_data_y[indices_unlabeled], Expert.predict , [0]*len(indices_unlabeled))\n",
        "        dataLoaderTrainLabeled = DataLoader(dataset=dataset_train_labeled, batch_size=128, shuffle=True,  num_workers=0, pin_memory=True)\n",
        "        dataLoaderTrainUnlabeled = DataLoader(dataset=dataset_train_unlabeled, batch_size=128, shuffle=True,  num_workers=0, pin_memory=True)\n",
        "        \n",
        "        run_expert(model_expert_OG, EPOCH_TRAIN, dataLoaderTrainLabeled, dataLoaderTrainLabeled)\n",
        "        metrics_confidence = metrics_print_2step(model, model_expert_OG, Expert.predict, 10, dataLoaderVal)\n",
        "        error_confidence.append(metrics_confidence['system accuracy'])\n",
        "        data_sizes.append((round+1)*BATCH_SIZE_AL + INITIAL_SIZE)\n",
        "    error_confidenceensemble_trials.append(error_confidence)"
      ],
      "metadata": {
        "id": "BLq393Sl7GO2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# confidence of rejector selection\n",
        "from scipy.stats import entropy\n",
        "\n",
        "\n",
        "def get_least_confident_rejector(model, model_exp, data_loader, budget):\n",
        "    '''\n",
        "    based on entropy score, can chagnge, but make sure to get max or min accordingly\n",
        "    '''\n",
        "    uncertainty_estimates = []\n",
        "    indices_all = []\n",
        "    for data in data_loader:\n",
        "        images, labels, expert_preds, indices, _ = data\n",
        "        images, labels, expert_preds = images.to(device), labels.to(device), expert_preds.to(device)\n",
        "        outputs_mod = model(images)\n",
        "        outputs_exp = model_exp(images)\n",
        "        batch_size = outputs_mod.size()[0]  \n",
        "        _, predicted = torch.max(outputs_mod.data, 1)\n",
        "        for i in range(0, batch_size):\n",
        "            output_i=  outputs_mod.data[i].cpu().numpy()[predicted[i].item()]\n",
        "            output_exp = outputs_exp.data[i][1].item() \n",
        "            #r_score = -abs(output_exp - output_i) +  entropy(outputs_exp.data[i].cpu().numpy())\n",
        "            r_score = 1 - output_i\n",
        "            r_actual = (output_exp >= output_i)\n",
        "            error_score = 0\n",
        "            ai_is_correct = (predicted[i].item() != labels[i].item()) * 1.0\n",
        "            error_score = ai_is_correct * (1 - output_i)\n",
        "            #if r_actual == 1:\n",
        "            #    error_score = (expert_preds[i].item() != labels[i].item())*1.0 + entropy(output_exp)\n",
        "            #else:\n",
        "            #    error_score = (predicted[i].item() != labels[i].item())*1.0 + entropy(output_i)\n",
        "            uncertainty_estimates.append(r_score)\n",
        "            indices_all.append(indices[i].item())\n",
        "    uncertainty_estimates = np.array(uncertainty_estimates)\n",
        "    indices_all = np.array(indices_all)\n",
        "    top_budget_indices = np.argsort(uncertainty_estimates)[-budget:]\n",
        "    print(uncertainty_estimates[top_budget_indices])\n",
        "    actual_indices = indices_all[top_budget_indices]\n",
        "    return actual_indices\n",
        "\n",
        "error_confidence_rejector_trials = []\n",
        "for trial in range(MAX_TRIALS):\n",
        "    all_indices = list(range(len(train_dataset.indices)))\n",
        "    all_data_x = np.array(train_dataset.dataset.data)[train_dataset.indices]\n",
        "    all_data_y = np.array(train_dataset.dataset.targets)[train_dataset.indices]\n",
        "\n",
        "    indices_labeled  = Intial_random_set\n",
        "    indices_unlabeled= list(set(all_indices) - set(indices_labeled))\n",
        "\n",
        "    dataset_train_labeled = CifarExpertDataset(all_data_x[indices_labeled], all_data_y[indices_labeled], Expert.predict , [1]*len(indices_labeled), indices_labeled)\n",
        "    dataset_train_unlabeled = CifarExpertDataset(all_data_x[indices_unlabeled], all_data_y[indices_unlabeled], Expert.predict , [0]*len(indices_unlabeled), indices_unlabeled)\n",
        "\n",
        "    dataLoaderTrainLabeled = DataLoader(dataset=dataset_train_labeled, batch_size=128, shuffle=True,  num_workers=0, pin_memory=True)\n",
        "    dataLoaderTrainUnlabeled = DataLoader(dataset=dataset_train_unlabeled, batch_size=128, shuffle=True,  num_workers=0, pin_memory=True)\n",
        "\n",
        "    model_expert = NetSimple(2, 100, 100, 1000,500).to(device)\n",
        "    run_expert(model_expert, EPOCH_TRAIN, dataLoaderTrainLabeled, dataLoaderTrainLabeled) \n",
        "    data_sizes = []\n",
        "    error_confidence_rejector = []\n",
        "    data_sizes.append(INITIAL_SIZE)\n",
        "    metrics_confidence = metrics_print_2step(model, model_expert, Expert.predict, 10, dataLoaderVal)\n",
        "    error_confidence_rejector.append(metrics_confidence['system accuracy'])\n",
        "    for round in range(MAX_ROUNDS):\n",
        "        print(f'\\n \\n Round {round} \\n \\n')\n",
        "        #if round % 2 == 1:\n",
        "        #    indices_confidence = random.sample(indices_unlabeled, BATCH_SIZE_AL)\n",
        "        #else:\n",
        "        indices_confidence = get_least_confident_rejector(model, model_expert, dataLoaderTrainUnlabeled, BATCH_SIZE_AL)\n",
        "        indices_labeled  = indices_labeled + list(indices_confidence) \n",
        "        indices_unlabeled= list(set(all_indices) - set(indices_labeled))\n",
        "        dataset_train_labeled = CifarExpertDataset(all_data_x[indices_labeled], all_data_y[indices_labeled], Expert.predict , [1]*len(indices_labeled))\n",
        "        dataset_train_unlabeled = CifarExpertDataset(all_data_x[indices_unlabeled], all_data_y[indices_unlabeled], Expert.predict , [0]*len(indices_unlabeled))\n",
        "        dataLoaderTrainLabeled = DataLoader(dataset=dataset_train_labeled, batch_size=128, shuffle=True,  num_workers=0, pin_memory=True)\n",
        "        dataLoaderTrainUnlabeled = DataLoader(dataset=dataset_train_unlabeled, batch_size=128, shuffle=True,  num_workers=0, pin_memory=True)\n",
        "        #model_expert = NetSimple(2, 100, 100, 1000,500).to(device)\n",
        "        run_expert(model_expert, EPOCH_TRAIN, dataLoaderTrainLabeled, dataLoaderTrainLabeled)\n",
        "\n",
        "        metrics_confidence = metrics_print_2step(model, model_expert, Expert.predict, 10, dataLoaderVal)\n",
        "        error_confidence_rejector.append(metrics_confidence['system accuracy'])\n",
        "        data_sizes.append((round+1)*BATCH_SIZE_AL + INITIAL_SIZE)\n",
        "    error_confidence_rejector_trials.append(error_confidence_rejector)"
      ],
      "metadata": {
        "id": "1CVmrPUmoXe3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# confidence of rejector selection but with L_CE loss\n",
        "\n",
        "all_indices = list(range(len(train_dataset.indices)))\n",
        "all_data_x = np.array(train_dataset.dataset.data)[train_dataset.indices]\n",
        "all_data_y = np.array(train_dataset.dataset.targets)[train_dataset.indices]\n",
        "\n",
        "intial_random_set = random.sample(all_indices, INITIAL_SIZE)\n",
        "indices_labeled  = intial_random_set\n",
        "indices_unlabeled= list(set(all_indices) - set(indices_labeled))\n",
        "\n",
        "dataset_train_labeled = CifarExpertDataset(all_data_x[indices_labeled], all_data_y[indices_labeled], Expert.predict , [1]*len(indices_labeled), indices_labeled)\n",
        "dataset_train_unlabeled = CifarExpertDataset(all_data_x[indices_unlabeled], all_data_y[indices_unlabeled], Expert.predict , [0]*len(indices_unlabeled), indices_unlabeled)\n",
        "\n",
        "dataLoaderTrainLabeled = DataLoader(dataset=dataset_train_labeled, batch_size=128, shuffle=True,  num_workers=0, pin_memory=True)\n",
        "dataLoaderTrainUnlabeled = DataLoader(dataset=dataset_train_unlabeled, batch_size=128, shuffle=True,  num_workers=0, pin_memory=True)\n",
        "\n",
        "model_lce = NetSimple(n_dataset + 1, 100, 100, 1000,500).to(device)\n",
        "run_reject_class(model_lce, 10, dataLoaderTrainUnlabeled, dataLoaderVal)\n",
        "\n",
        "#run_reject(model, 10, Expert.predict, 70,0.5, dataLoaderTrain, dataLoaderVal)\n",
        "\n",
        "\n",
        "def get_least_confident_rejector_uncertain(model, data_loader, budget):\n",
        "    '''\n",
        "    based on entropy score, can chagnge, but make sure to get max or min accordingly\n",
        "    '''\n",
        "    uncertainty_estimates = []\n",
        "    indices_all = []\n",
        "    for data in data_loader:\n",
        "        images, labels, expert_preds, indices, _ = data\n",
        "        images, labels, expert_preds = images.to(device), labels.to(device), expert_preds.to(device)\n",
        "        outputs_mod = model(images)\n",
        "        batch_size = outputs_mod.size()[0]  \n",
        "        _, predicted = torch.max(outputs_mod.data, 1)\n",
        "        for i in range(0, batch_size):\n",
        "            output_i=  outputs_mod.data[i].cpu().numpy()[predicted[i].item()]\n",
        "            output_exp = outputs_exp.data[i][1].item()\n",
        "            all_output_exp =  outputs_exp.data[i].cpu().numpy()\n",
        "            entropy_exp = entropy(all_output_exp)\n",
        "            r_score = -abs(output_exp - output_i) + entropy_exp\n",
        "            uncertainty_estimates.append(r_score)\n",
        "            indices_all.append(indices[i].item())\n",
        "    indices_all = np.array(indices_all)\n",
        "    top_budget_indices = np.argsort(uncertainty_estimates)[-budget:]\n",
        "    actual_indices = indices_all[top_budget_indices]\n",
        "    return actual_indices\n",
        "\n",
        "#model_expert = NetSimple(2, 100, 100, 1000,500).to(device)\n",
        "#run_expert(model_expert, 10, dataLoaderTrainLabeled, dataLoaderTrainLabeled) \n",
        "data_sizes = []\n",
        "errors_LCE = []\n",
        "data_sizes.append(INITIAL_SIZE)\n",
        "metrics_confidence = metrics_print(model_lce, Expert.predict, n_dataset, dataLoaderVal)\n",
        "errors_LCE.append(metrics_confidence['system accuracy'])\n",
        "for round in range(MAX_ROUNDS):\n",
        "    print(f'\\n \\n Round {round} \\n \\n')\n",
        "    #indices_confidence = get_least_confident_rejector(model, model_expert, dataLoaderTrainUnlabeled, BATCH_SIZE_AL)\n",
        "    indices_confidence = random.sample(indices_unlabeled, BATCH_SIZE_AL)\n",
        "    indices_labeled  = indices_labeled + list(indices_confidence) \n",
        "    indices_unlabeled= list(set(all_indices) - set(indices_labeled))\n",
        "    dataset_train_labeled = CifarExpertDataset(all_data_x[indices_labeled], all_data_y[indices_labeled], Expert.predict , [1]*len(indices_labeled))\n",
        "    dataset_train_unlabeled = CifarExpertDataset(all_data_x[indices_unlabeled], all_data_y[indices_unlabeled], Expert.predict , [0]*len(indices_unlabeled))\n",
        "    dataLoaderTrainLabeled = DataLoader(dataset=dataset_train_labeled, batch_size=128, shuffle=True,  num_workers=0, pin_memory=True)\n",
        "    dataLoaderTrainUnlabeled = DataLoader(dataset=dataset_train_unlabeled, batch_size=128, shuffle=True,  num_workers=0, pin_memory=True)\n",
        "    \n",
        "    #run_expert(model_expert, 20, dataLoaderTrainLabeled, dataLoaderVal)\n",
        "    run_reject(model_lce, 10, Expert.predict, 1, 0.5, dataLoaderTrainLabeled, dataLoaderTrainLabeled)\n",
        "    metrics_confidence = metrics_print(model_lce, Expert.predict, n_dataset, dataLoaderVal)\n",
        "    errors_LCE.append(metrics_confidence['system accuracy'])\n",
        "    data_sizes.append((round+1)*BATCH_SIZE_AL + INITIAL_SIZE)"
      ],
      "metadata": {
        "id": "H3Me6cnD7J7_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# teaching baseline\n",
        "import copy\n",
        "from scipy.stats import entropy\n",
        "all_indices = list(range(len(train_dataset.indices)))\n",
        "all_data_x = np.array(train_dataset.dataset.data)[train_dataset.indices]\n",
        "all_data_y = np.array(train_dataset.dataset.targets)[train_dataset.indices]\n",
        "\n",
        "intial_random_set = random.sample(all_indices, INITIAL_SIZE)\n",
        "indices_labeled  = intial_random_set\n",
        "indices_unlabeled= list(set(all_indices) - set(indices_labeled))\n",
        "\n",
        "dataset_train_labeled = CifarExpertDataset(all_data_x[indices_labeled], all_data_y[indices_labeled], Expert.predict , [1]*len(indices_labeled), indices_labeled)\n",
        "dataset_train_unlabeled = CifarExpertDataset(all_data_x[indices_unlabeled], all_data_y[indices_unlabeled], Expert.predict , [0]*len(indices_unlabeled), indices_unlabeled)\n",
        "\n",
        "dataLoaderTrainLabeled = DataLoader(dataset=dataset_train_labeled, batch_size=128, shuffle=True,  num_workers=0, pin_memory=True)\n",
        "dataLoaderTrainUnlabeled = DataLoader(dataset=dataset_train_unlabeled, batch_size=128, shuffle=True,  num_workers=0, pin_memory=True)\n",
        "\n",
        "\n",
        "model_expert = NetSimple(2, 100, 100, 1000,500).to(device)\n",
        "run_expert(model_expert, 10, dataLoaderTrainLabeled, dataLoaderTrainLabeled) \n",
        "data_sizes = []\n",
        "errors_teaching = []\n",
        "data_sizes.append(INITIAL_SIZE)\n",
        "metrics_confidence = metrics_print_2step(model, model_expert, Expert.predict, 10, dataLoaderVal)\n",
        "errors_teaching.append(metrics_confidence['system accuracy'])\n",
        "RANDOM_SEARCH_SIZE = 30\n",
        "for round in range(MAX_ROUNDS):\n",
        "    print(f'\\n \\n Round {round} \\n \\n')\n",
        "    random_sets = []\n",
        "    best_set_score = 0\n",
        "    best_set = []\n",
        "    saved_expert_model = copy.deepcopy(model_expert.state_dict())\n",
        "    for trial_set in range(RANDOM_SEARCH_SIZE):\n",
        "        model_expert.load_state_dict(saved_expert_model)\n",
        "        random_set = random.sample(indices_unlabeled, BATCH_SIZE_AL)\n",
        "        random_sets.append(random_set)\n",
        "        indices_labeled_trial  = indices_labeled + list(random_set) \n",
        "        indices_unlabeled_trial= list(set(all_indices) - set(indices_labeled))\n",
        "        dataset_train_labeled = CifarExpertDataset(all_data_x[indices_labeled], all_data_y[indices_labeled], Expert.predict , [1]*len(indices_labeled))\n",
        "        dataset_train_unlabeled = CifarExpertDataset(all_data_x[indices_unlabeled], all_data_y[indices_unlabeled], Expert.predict , [0]*len(indices_unlabeled))\n",
        "        dataLoaderTrainLabeled = DataLoader(dataset=dataset_train_labeled, batch_size=128, shuffle=True,  num_workers=0, pin_memory=True)\n",
        "        dataLoaderTrainUnlabeled = DataLoader(dataset=dataset_train_unlabeled, batch_size=128, shuffle=True,  num_workers=0, pin_memory=True)\n",
        "        run_expert(model_expert, 20, dataLoaderTrainLabeled, dataLoaderVal)\n",
        "        metrics_confidence = metrics_print_2step(model, model_expert, Expert.predict, 10, dataLoaderVal)\n",
        "        error_random_set = metrics_confidence['system accuracy']\n",
        "        if error_random_set >= best_set_score:\n",
        "            best_set_score = error_random_set\n",
        "            best_set = random_set\n",
        "\n",
        "    model_expert.load_state_dict(saved_expert_model)\n",
        "    indices_labeled  = indices_labeled + list(best_set) \n",
        "    indices_unlabeled = list(set(all_indices) - set(indices_labeled))\n",
        "    dataset_train_labeled = CifarExpertDataset(all_data_x[indices_labeled], all_data_y[indices_labeled], Expert.predict , [1]*len(indices_labeled))\n",
        "    dataset_train_unlabeled = CifarExpertDataset(all_data_x[indices_unlabeled], all_data_y[indices_unlabeled], Expert.predict , [0]*len(indices_unlabeled))\n",
        "    dataLoaderTrainLabeled = DataLoader(dataset=dataset_train_labeled, batch_size=128, shuffle=True,  num_workers=0, pin_memory=True)\n",
        "    dataLoaderTrainUnlabeled = DataLoader(dataset=dataset_train_unlabeled, batch_size=128, shuffle=True,  num_workers=0, pin_memory=True)\n",
        "    run_expert(model_expert, 20, dataLoaderTrainLabeled, dataLoaderVal)\n",
        "    metrics_confidence = metrics_print_2step(model, model_expert, Expert.predict, 10, dataLoaderVal)\n",
        "    errors_teaching.append(metrics_confidence['system accuracy'])\n",
        "    data_sizes.append((round+1)*BATCH_SIZE_AL + INITIAL_SIZE)"
      ],
      "metadata": {
        "id": "xXQt1d6Yq4cd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# plot"
      ],
      "metadata": {
        "id": "FW0wMpIABppB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt install texlive-fonts-recommended texlive-fonts-extra cm-super dvipng\n",
        "import matplotlib\n",
        "\n",
        "matplotlib.rcParams['pdf.fonttype'] = 42\n",
        "matplotlib.rcParams['ps.fonttype'] = 42\n",
        "plt.rc('text', usetex=False)\n",
        "plt.rc('font', family='serif')\n",
        "def get_conf_interval(arr):\n",
        "    alpha_level = 0.4\n",
        "    err  = st.t.interval(alpha_level, len(arr)-1, loc=np.mean(arr), scale=st.sem(arr))[1]/2  - st.t.interval(alpha_level, len(arr)-1, loc=np.mean(arr), scale=st.sem(arr))[0]/2 \n",
        "    return err"
      ],
      "metadata": {
        "id": "Jk7MhFZdUExS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "#avgs_rand = [np.average([scores_oracle[triall] - scores_lime[triall][i] for triall in range(actual_max_trials)]) for i in range(len(teaching_sizes))]\n",
        "#stds_rand = [np.std([scores_oracle[triall] -scores_lime[triall][i] for triall in range(actual_max_trials)]) for i in range(len(teaching_sizes))]\n",
        "#plt.errorbar(list(range(1,len(teaching_sizes)+1)),  avgs_rand, yerr=stds_rand, label=f'random')\n",
        "avgs_rand = [np.average([error_random_trials[triall][i]  for triall in range(MAX_TRIALS)]) for i in range(MAX_ROUNDS + 1)]\n",
        "stds_rand = [np.std([error_random_trials[triall][i]  for triall in range(MAX_TRIALS)]) for i in range(MAX_ROUNDS + 1)]\n",
        "plt.errorbar(data_sizes,  avgs_rand, yerr=stds_rand, marker = \"+\",  label=f'Random')\n",
        "'''\n",
        "avgs_rand = [np.average([error_confidence_rejector_trials[triall][i]  for triall in range(MAX_TRIALS)]) for i in range(MAX_ROUNDS + 1)]\n",
        "stds_rand = [np.std([error_confidence_rejector_trials[triall][i]  for triall in range(MAX_TRIALS)]) for i in range(MAX_ROUNDS + 1)]\n",
        "plt.errorbar(data_sizes,  avgs_rand, yerr=stds_rand, marker = \"+\",  label=f'Rejector')\n",
        "\n",
        "avgs_rand = [np.average([error_confidence_trials[triall][i]  for triall in range(MAX_TRIALS)]) for i in range(MAX_ROUNDS+1)]\n",
        "stds_rand = [np.std([error_confidence_trials[triall][i]  for triall in range(MAX_TRIALS)]) for i in range(MAX_ROUNDS+1)]\n",
        "plt.errorbar(data_sizes,  avgs_rand, yerr=stds_rand, marker = \"o\",  label=f'Entropy Sampling')\n",
        "'''\n",
        "avgs_rand = [np.average([error_confidence_trials_LCE[triall][i]  for triall in range(MAX_TRIALS)]) for i in range(MAX_ROUNDS+1)]\n",
        "stds_rand = [np.std([error_confidence_trials_LCE[triall][i]  for triall in range(MAX_TRIALS)]) for i in range(MAX_ROUNDS+1)]\n",
        "plt.errorbar(data_sizes,  avgs_rand, yerr=stds_rand, marker = \"o\",  label=f'Entropy Sampling (Ensemble)')\n",
        "\n",
        "#plt.errorbar(data_sizes,  error_confidence_rejector, yerr=[0]*len(error_confidence_rejector), marker = \"+\",  label=f'Rejector Uncertainty')\n",
        "#plt.errorbar(data_sizes,  errors_LCE, yerr=[0]*len(errors_LCE), marker = \"o\",  label=f'LCE random')\n",
        "#plt.errorbar(data_sizes,  errors_teaching, yerr=[0]*len(error_confidence_rejector), marker = \"o\",  label=f'Teaching')\n",
        "\n",
        "\n",
        "ax = plt.gca()\n",
        "ax.spines['right'].set_visible(False)\n",
        "ax.spines['top'].set_visible(False)\n",
        "ax.get_xaxis().tick_bottom()    \n",
        "ax.get_yaxis().tick_left()   \n",
        "plt.grid()\n",
        "plt.legend(fontsize='large')\n",
        "plt.legend()\n",
        "plt.ylabel('System Accuracy',  fontsize='x-large')\n",
        "plt.xlabel('Acquired data size', fontsize='x-large')\n",
        "fig_size = plt.rcParams[\"figure.figsize\"]\n",
        "fig_size[0] = 6\n",
        "fig_size[1] = 4\n",
        "#plt.savefig(\"teaching_complexity_cifar10.pdf\", dpi = 1000)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 285
        },
        "id": "KXSoV9ERUJou",
        "outputId": "33b393cd-4aa8-4f2c-9217-080d17ea7b6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test on unlabeled and retrieve a score for each data point"
      ],
      "metadata": {
        "id": "8692xxBU6YmD"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "a7Aympk-qwGu",
        "iYllTn0F2iDg",
        "iqf_r99A1OcX",
        "s8bHnx2qbN2o"
      ],
      "name": "cifar_active_learning.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}