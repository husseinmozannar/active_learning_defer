{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WvmtEh42mViU"
   },
   "source": [
    "# Imports and utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eJx9ma02mX5Z"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import argparse\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yXTauo6LmYX8"
   },
   "outputs": [],
   "source": [
    "from neural_network import *\n",
    "from utils import *\n",
    "from metrics import *\n",
    "from training_helpers import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RePVfWZwmfgd",
    "outputId": "2727c25a-7e49-44c5-dc5d-0ad4f3d88c1a"
   },
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lLSngoolmi2d"
   },
   "outputs": [],
   "source": [
    "k = 5 # number of classes expert can predict\n",
    "n_dataset = 10\n",
    "Expert = synth_expert(k, n_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SBJvS3QpmkQl",
    "outputId": "49302cef-c1de-46cc-a8ee-e15b13be65c0"
   },
   "outputs": [],
   "source": [
    "use_data_aug = False\n",
    "n_dataset = 10  # cifar-10\n",
    "normalize = transforms.Normalize(mean=[x / 255.0 for x in [125.3, 123.0, 113.9]],\n",
    "                                    std=[x / 255.0 for x in [63.0, 62.1, 66.7]])\n",
    "\n",
    "if use_data_aug:\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Lambda(lambda x: F.pad(x.unsqueeze(0),\n",
    "                                            (4, 4, 4, 4), mode='reflect').squeeze()),\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.RandomCrop(32),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ])\n",
    "else:\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ])\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    normalize\n",
    "])\n",
    "\n",
    "if n_dataset == 10:\n",
    "    dataset = 'cifar10'\n",
    "elif n_dataset == 100:\n",
    "    dataset = 'cifar100'\n",
    "\n",
    "kwargs = {'num_workers': 0, 'pin_memory': True}\n",
    "\n",
    "\n",
    "train_dataset_all = datasets.__dict__[dataset.upper()]('../data', train=True, download=True,\n",
    "                                                        transform=transform_train)\n",
    "train_size = int(0.90 * len(train_dataset_all))\n",
    "test_size = len(train_dataset_all) - train_size\n",
    "\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(train_dataset_all, [train_size, test_size])\n",
    "#train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "#                                           batch_size=128, shuffle=True, **kwargs)\n",
    "#val_loader = torch.utils.data.DataLoader(val_dataset,\n",
    "#                                            batch_size=128, shuffle=True, **kwargs)\n",
    "\n",
    "\n",
    "normalize = transforms.Normalize(mean=[x / 255.0 for x in [125.3, 123.0, 113.9]],\n",
    "                                 std=[x / 255.0 for x in [63.0, 62.1, 66.7]])\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True}\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    normalize\n",
    "])\n",
    "test_dataset = datasets.__dict__[\"cifar10\".upper()]('../data', train=False, transform=transform_test, download=True)\n",
    "#test_loader = torch.utils.data.DataLoader(\n",
    "#    datasets.__dict__[\"cifar100\".upper()]('../data', train=False, transform=transform_test, download=True),\n",
    "#    batch_size=128, shuffle=True, **kwargs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0YmBYZg1mlo_"
   },
   "outputs": [],
   "source": [
    "class CifarExpertDataset(Dataset):\n",
    "    def __init__(self, images, targets, expert_fn, labeled, indices = None):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        self.images = images\n",
    "        self.targets = np.array(targets)\n",
    "        self.expert_fn = expert_fn\n",
    "        self.labeled = np.array(labeled)\n",
    "        self.expert_preds = np.array(expert_fn(None, torch.FloatTensor(targets)))\n",
    "        for i in range(len(self.expert_preds)):\n",
    "            if self.labeled[i] == 0:\n",
    "                self.expert_preds[i] = -1 # not labeled by expert\n",
    "        if indices != None:\n",
    "            self.indices = indices\n",
    "        else:\n",
    "            self.indices = np.array(list(range(len(self.targets))))\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Take the index of item and returns the image, label, expert prediction and index in original dataset\"\"\"\n",
    "        label = self.targets[index]\n",
    "        image = transform_test(self.images[index])\n",
    "        expert_pred = self.expert_preds[index]\n",
    "        indice = self.indices[index]\n",
    "        labeled = self.labeled[index]\n",
    "        return torch.FloatTensor(image), label, expert_pred, indice, labeled\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rI_kDhh5mm2b"
   },
   "outputs": [],
   "source": [
    "dataset_train = CifarExpertDataset(np.array(train_dataset.dataset.data)[train_dataset.indices], np.array(train_dataset.dataset.targets)[train_dataset.indices], Expert.predict , [1]*len(train_dataset.indices))\n",
    "dataset_val = CifarExpertDataset(np.array(val_dataset.dataset.data)[val_dataset.indices], np.array(val_dataset.dataset.targets)[val_dataset.indices], Expert.predict , [1]*len(val_dataset.indices))\n",
    "dataset_test = CifarExpertDataset(test_dataset.data , test_dataset.targets, Expert.predict , [1]*len(test_dataset.targets))\n",
    "\n",
    "dataLoaderTrain = DataLoader(dataset=dataset_train, batch_size=128, shuffle=True,  num_workers=0, pin_memory=True)\n",
    "dataLoaderVal = DataLoader(dataset=dataset_val, batch_size=128, shuffle=False,  num_workers=0, pin_memory=True)\n",
    "dataLoaderTest = DataLoader(dataset=dataset_test, batch_size=128, shuffle=False,  num_workers=0, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t5utTPLKmnNX"
   },
   "source": [
    "# Figure 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "amGgRqfD8YFD"
   },
   "outputs": [],
   "source": [
    "model_expert = NetSimple(2,  100,100,1000,500).to(device)\n",
    "run_expert(model_expert,60, dataLoaderTrain, dataLoaderVal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AG8OYFfRmoEH"
   },
   "outputs": [],
   "source": [
    "params = [[1,1,50,25], [3,3,50,25], [4,4,80,40], [6,6,100,50],[12,12,100,50], [20,20,100,50],[100,100,500,250],[100,100,1000,500]]\n",
    "param_model_params = []\n",
    "MAX_TRIALS = 10\n",
    "EPOCHS = 60\n",
    "EPOCHS_ALPHA = 15\n",
    "joint_results = []\n",
    "seperate_results = []\n",
    "\n",
    "\n",
    "for trial in range(MAX_TRIALS):\n",
    "    joint = []\n",
    "    seperate = []\n",
    "    for param in params:\n",
    "        print(f'\\n \\n param {param} \\n \\n')\n",
    "        print(f' \\n Joint \\n')\n",
    "\n",
    "        net_h_params = [10] + param\n",
    "        net_r_params = [1] + [100,100,1000,500]\n",
    "        model_2_r = NetSimpleRejector(net_h_params, net_r_params).to(device)\n",
    "        model_dict = run_reject(model_2_r, 10, Expert.predict, EPOCHS, 1, dataLoaderTrain, dataLoaderVal, True)\n",
    "        best_score = 0\n",
    "        best_model = None\n",
    "        for alpha in [0, 0.1,  0.5, 1]:\n",
    "            print(f'alpha {alpha}')\n",
    "            model_2_r.load_state_dict(model_dict)\n",
    "            model_dict_alpha = run_reject(model_2_r, 10, Expert.predict, EPOCHS_ALPHA, alpha, dataLoaderTrain, dataLoaderVal, True, 1)\n",
    "            model_2_r.load_state_dict(model_dict_alpha)\n",
    "            score = metrics_print(model_2_r, Expert.predict, n_dataset, dataLoaderTest)['system accuracy']\n",
    "            if score >= best_score:\n",
    "                best_score =  score\n",
    "                best_model = model_dict_alpha\n",
    "\n",
    "        model_2_r.load_state_dict(best_model)\n",
    "        joint.append(metrics_print(model_2_r, Expert.predict, n_dataset, dataLoaderTest)['system accuracy'])\n",
    "        print(f' \\n Seperate \\n')\n",
    "        # seperate\n",
    "        model_class = NetSimple(n_dataset,  param[0], param[1], param[2], param[3]).to(device)\n",
    "        num_params = sum(dict((p.data_ptr(), p.numel()) for p in model_class.parameters()).values())\n",
    "        if trial == 0:\n",
    "            param_model_params.append(num_params)\n",
    "        run_reject_class(model_class, EPOCHS, dataLoaderTrain, dataLoaderVal)\n",
    "        seperate.append(metrics_print_2step(model_class, model_expert, Expert.predict, 10, dataLoaderTest)['system accuracy'])\n",
    "    \n",
    "    joint_results.append(joint)\n",
    "    seperate_results.append(seperate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 450
    },
    "id": "yTI8fdrtmrkW",
    "outputId": "8571ff26-cc4d-43e9-bca7-2b84581830fc"
   },
   "outputs": [],
   "source": [
    "model_sizes = list(range(len(params)))\n",
    "'''\n",
    "avgs_rand = [np.average([seperate_results[triall][i] for triall in range(MAX_TRIALS)]) for i in range(len(model_sizes))]\n",
    "stds_rand = [np.std([seperate_results[triall][i] for triall in range(MAX_TRIALS)]) for i in range(len(model_sizes))]\n",
    "plt.errorbar(model_sizes,  avgs_rand, yerr=stds_rand, marker = \"o\",  label=f'Seperate')\n",
    "\n",
    "\n",
    "avgs_rand = [np.average([joint_results[triall][i] for triall in range(MAX_TRIALS)]) for i in range(len(model_sizes))]\n",
    "stds_rand = [np.std([joint_results[triall][i] for triall in range(MAX_TRIALS)]) for i in range(len(model_sizes))]\n",
    "plt.errorbar(model_sizes,  avgs_rand, yerr=stds_rand, marker = \"x\",  label=f'Joint')\n",
    "'''\n",
    "\n",
    "avgs_rand = [np.average([joint_results[triall][i] - seperate_results[triall][i]  for triall in range(MAX_TRIALS)]) for i in range(len(model_sizes))]\n",
    "stds_rand = [np.std([joint_results[triall][i] - seperate_results[triall][i] for triall in range(MAX_TRIALS)]) for i in range(len(model_sizes))]\n",
    "plt.errorbar(param_model_params,  avgs_rand, yerr=stds_rand, marker = \"x\",  label=f' Joint - Staged')\n",
    "\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.get_xaxis().tick_bottom()    \n",
    "ax.get_yaxis().tick_left()   \n",
    "plt.grid()\n",
    "plt.legend(fontsize='xx-large')\n",
    "plt.legend()\n",
    "plt.ylabel('Gap in System Accuracy (%)', fontsize='xx-large')\n",
    "plt.xlabel('Classifier Class Complexity (number of parameters)', fontsize='xx-large')\n",
    "fig_size = plt.rcParams[\"figure.figsize\"]\n",
    "fig_size[0] = 6\n",
    "fig_size[1] = 4.2\n",
    "plt.xscale('log')\n",
    "plt.savefig(\"difference_model_complexity_params_log.pdf\", dpi = 1000, bbox_inches='tight')\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "WvmtEh42mViU"
   ],
   "name": "joint_v_seperate.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
